{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OqnsxUxSi7Fx"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additional Tasks\n",
        "Task 5: Transformer-Based Embeddings and Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading the dataset into Google Collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELEWHiGHjYnx",
        "outputId": "e706ce4d-87c3-495b-87d9-910b8c2640b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/emineyetm/fake-news-detection-datasets\n",
            "License(s): unknown\n",
            "fake-news-detection-datasets.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d emineyetm/fake-news-detection-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unzip the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GzHXjBy4jZXN"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/fake-news-detection-datasets.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8zjcOEZhjcka"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk_gmaccjevo",
        "outputId": "64d2a287-78c2-4bfb-adf5-c88b236dcdfd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pekbLrx4jhh_"
      },
      "outputs": [],
      "source": [
        "# Define the file paths\n",
        "true_news_path = '/content/News _dataset/True.csv'\n",
        "fake_news_path = '/content/News _dataset/Fake.csv'\n",
        "\n",
        "# Read the datasets\n",
        "true_news_df = pd.read_csv(true_news_path)\n",
        "fake_news_df = pd.read_csv(fake_news_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "9ajjajc0jjaj",
        "outputId": "b1c54159-4160-4280-ce34-00e6237c225c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True News Dataset:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"true_news_df\",\n  \"rows\": 21417,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20826,\n        \"samples\": [\n          \"German, Turkish foreign ministers meet after detainee released\",\n          \"Kremlin calls North Korea's latest missile launch another 'provocation'\",\n          \"Transgender soldiers, veterans shaken by Trump's ban on their service\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21192,\n        \"samples\": [\n          \"WASHINGTON (Reuters) - A majority of the U.S. Senate on Tuesday backed a new round of disaster aid to help Puerto Rico and several states recover from damage from hurricanes and wildfires. The legislation would provide $36.5 billion in emergency relief as Puerto Rico in particular struggles to regain electricity and other basic services following destructive hurricanes. The House of Representatives approved the bill earlier this month. The Trump administration already has indicated it will seek another round of emergency relief from Congress. \",\n          \"BUDAPEST (Reuters) - About a thousand Hungarians protested on Friday against a crackdown on the main opposition party Jobbik which has been  threatened by a record political campaign fine that the party leader describes as a  death sentence  for democracy. Despite the gloomy rhetoric and Jobbik saying it was fighting for survival, support for the demonstration was well down on other similar rallies over the past year. Hungarians will vote for a new parliament in April and Prime Minister Viktor Orban s conservative, anti-migrant Fidesz party is far ahead in the polls, with Jobbik its nearest rival. Jobbik, once on the far right, has turned toward the center in a bid to attract more support and is now campaigning nationwide against Orban, depicting him as the leader of a criminal gang. Orban, rejecting the charges, says his financial standing is  an open book . Last week the state audit office (ASZ) ruled Jobbik had bought political posters far below market prices, breaching rules on political funding, then it slapped a 663 million forint ($2.5 million) penalty on the party. The protesters, waving Jobbik flags and posters deriding the ruling elite, gathered outside the headquarters of Orban s Fidesz party.  What we see unfolding is not an audit office investigation. It is not an official penalty. This is a death sentence with Jobbik s name on it. But in reality, it is a death sentence for Hungarian democracy,  Jobbiik leader Gabor Vona told the crowd. A government spokesman could not comment immediately on his remarks. ASZ chairman Laszlo Domokos is a former Fidesz lawmaker, whom Jobbik and other critics accuse of making decisions in favor of Orban. The audit office denies that. On Friday, ASZ again called on Jobbik to submit information that would challenge its findings, saying it acted fully within its rights throughout the probe. The ruling Fidesz party and the government have denied any involvement in the ASZ probe.  This case has nothing to do with the election campaign,  Orban aide Janos Lazar said on Thursday. For over a year Fidesz has targeted Jobbik, whose move to the center could upend the longstanding status quo of a dominant Fidesz with weaker opponents to its left and its right, said analyst Zoltan Novak at the Centre for Fair Political Analysis. Gyorgy Illes, a 67-year-old pensioner attending the rally, said he used to be a Socialist supporter but got disillusioned as the party struggled to overcome its internal divisions.  This ASZ probe is a clear sign that Orban is way past any remedy. It is a ruthless attack on everything we hold dear. Democracy, the rule of law, equality, you name it,  he said. \",\n          \"BEIJING/TAIPEI (Reuters) - China accused the United States on Thursday of interfering in its internal affairs and said it had lodged a complaint after U.S. President Donald Trump signed into law an act laying the groundwork for possible U.S. navy visits to self-ruled Taiwan. Tensions have risen in recent days after a senior Chinese diplomat threatened China would invade Taiwan if any U.S. warships made port visits to the island which China claims as its own territory. On Monday, Chinese jets carried out  island encirclement patrols  around Taiwan, with state media showing pictures of bombers with cruise missiles slung under their wings as they carried out the exercise. On Tuesday, Trump signed into law the National Defense Authorization Act for the 2018 fiscal year, which authorizes the possibility of mutual visits by navy vessels between Taiwan and the United States. Such visits would be the first since the United States ended formal diplomatic relations with Taiwan in 1979 and established ties with Beijing. Chinese Foreign Ministry spokesman Lu Kang said while the Taiwan sections of the law were not legally binding, they seriously violate the  One China  policy and  constitute an interference in China s internal affairs .   China is resolutely opposed to this, and we have already lodged stern representations with the U.S. government,  Lu told a daily news briefing. China is firmly opposed to any official exchanges, military contact, or arms sales between Taiwan and the United States, he added.  Proudly democratic Taiwan has become increasingly concerned with the ramped up Chinese military presence, that has included several rounds of Chinese air force drills around the island in recent months.  Taiwan is confident of its defenses and responded quickly to the Chinese air force drills this week, its government said, denouncing the rise in China s military deployments as irresponsible. Taiwan presidential spokesman Alex Huang, speaking to Taiwan media in comments reported late on Wednesday, said the defense ministry had kept a close watch on the patrols and responded immediately and properly. Taiwan  can ensure there are no concerns at all about national security, and people can rest assured , Huang said. Both sides of the narrow Taiwan Strait, which separates Taiwan from its giant neighbor, have a responsibility to protect peace and stability, he added.  Such a raised military posture that may impact upon and harm regional peace and stability and cross-strait ties does not give a feeling of responsibility, and the international community does not look favorably upon this,  Huang was quoted as saying. Relations have soured considerably since Tsai Ing-wen, who leads Taiwan s independence-leaning Democratic Progressive Party, won presidential elections last year. China suspects Tsai wants to declare the island s formal independence, a red line for Beijing. Tsai says she wants to maintain peace with China but will defend Taiwan s security. Taiwan is well equipped with mostly U.S. weapons but has been pressing for more advanced equipment to deal with what it sees as a rising threat from China. The United States is bound by law to provide the island with the means to defend itself. China has never renounced the use of force to bring Taiwan under its control. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"worldnews\",\n          \"politicsNews\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 716,\n        \"samples\": [\n          \"September 2, 2017 \",\n          \"February 2, 2017 \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "true_news_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-164ad8cd-d0d3-4547-8b24-3192a36fdfba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 30, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-164ad8cd-d0d3-4547-8b24-3192a36fdfba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-164ad8cd-d0d3-4547-8b24-3192a36fdfba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-164ad8cd-d0d3-4547-8b24-3192a36fdfba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dcb51d29-3d18-412f-8d8f-d59782027de3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dcb51d29-3d18-412f-8d8f-d59782027de3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dcb51d29-3d18-412f-8d8f-d59782027de3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0  As U.S. budget fight looms, Republicans flip t...   \n",
              "1  U.S. military to accept transgender recruits o...   \n",
              "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
              "3  FBI Russia probe helped by Australian diplomat...   \n",
              "4  Trump wants Postal Service to charge 'much mor...   \n",
              "\n",
              "                                                text       subject  \\\n",
              "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
              "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
              "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
              "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
              "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
              "\n",
              "                 date  \n",
              "0  December 31, 2017   \n",
              "1  December 29, 2017   \n",
              "2  December 31, 2017   \n",
              "3  December 30, 2017   \n",
              "4  December 29, 2017   "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print the contents of the True news dataset\n",
        "print(\"True News Dataset:\")\n",
        "true_news_df.head()  # Print the first 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "fPg1HfXdjlEc",
        "outputId": "69a1f865-42bc-4bdc-bfa8-131661c21b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fake News Dataset:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"fake_news_df\",\n  \"rows\": 23481,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17903,\n        \"samples\": [\n          \" Fox News Mocked Into Oblivion After This F*cking STUPID Attempt To Make Steve Bannon Look Sane (TWEETS)\",\n          \"BREAKING: FL GOV RICK SCOTT Calls for FBI Director to Resign\",\n          \" WATCH: Mike Pence\\u2019s Photo Op With Puerto Rico Survivors Just Went TERRIBLY Wrong (VIDEO)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17455,\n        \"samples\": [\n          \"The moral decay continues The Kapiolani Medical Center for Women and Children at the University of Hawaii is currently recruiting pregnant girls and women to participate in second-trimester abortions to measure their bleeding during the operation, with and without antihemorrhagic drugs. According to the Clinical Trials website, run by the National Institutes of Health, participants must be at least 14 years old and 18-24 weeks pregnant.The controversial study, led by Bliss Kaneshiro, MD and Kate Whitehouse, DO, will monitor bleeding during D&E abortions to determine the effects of the drug oxytocin, commonly used to minimize blood loss and decrease the risk of hemorrhage.The clinical trial, called  Effects of Oxytocin on Bleeding Outcomes during Dilation and Evacuation  began in October 2014 and is a collaboration between UH, Society of Family Planning and the University of Washington.The Society of Family Planning funds a number of similar research projects, such as experimenting with the dosage of Misoprostol, a uterine contracting agent, prior to surgical abortions at 13-18 weeks and exploring umbilical cord injections to produce fetal death prior to late-term abortions.In the UH study, researchers will carry out a  randomized, double-blinded, placebo-controlled trials,  to determine the effect of oxytocin s use on uterine bleeding, meaning that they will either provide or deny intravenous oxytocin to the women.Reports suggest that some doctors are concerned that withholding oxytocin during surgery may put patients, especially teen girls, at risk. This study is reminiscent of Nazi concentration camp experiments. I pity the poor women who are being treated like lab rats, especially those who are denied the drug to reduce hemorrhaging,  said Troy Newman, President of Operation Rescue.Dilation and evacuation abortions are surgical procedures that involve dismembering the pre-born baby with forceps, scraping the inside of the uterus with a curette to remove any residuals and finally suctioning out the womb to make sure the contents are completely removed.After the abortion, the corpse of the fetus is reassembled and examined to ensure everything was successfully removed and that the abortion was complete.The study is hoping to attract up to 166 test subjects and is expected to conclude in July 2015.Via: Breitbart News\",\n          \"CNN was quick to scoop up Corey Lewandowski after Donald Trump kicked him out of his role as campaign manager, but his first week on the job is going pretty much exactly how you would expect it to go   terribly.Not only has Lewandowski proven himself to be pretty much like a paid spokesman for Trump, but his defense of the disgraced GOP candidate isn t being received well. Earlier this week, Lewandowski revealed that he was under contract and couldn t criticize The Donald, even after being fired from the campaign. Today, Lewandowski got called out by Hillary Clinton surrogate Christine Quinn for hyping Trump up to be an expert on the Brexit decision   a suggestion that was clearly false.On Monday s edition of CNN s New Day, Lewandowski made another pathetic defense of Trump by trying to reframe the candidate s disgusting reaction to Brexit, where he mostly spoke about how much the decision would be good for his Scotland golf resort. Lewandowski s defense was: Obviously the U.S. dollar has become much stronger now against the British pound. If you re going to spend money in Europe, now would actually be a good time to go with the fall of the pound.What you have is a world view, so what you have is someone who is saying,  Let s look at this from the U.S. perspective. If you want to go and travel overseas   just from a monetary perspective   now is the right time to do that because what you re getting is more for your dollar. Quinn wasn t having it. She ripped into Lewandowski, firing back, Donald Trump is not running to be travel agent of the world, he s running to be president of the United States.  She continued: What he said wasn t a commentary on international markets, it was,  When the pound goes down, more people will come to my golf course. Donald Trump s main concern isn t the international markets, it isn t the impact that Brexit will have on hard working Americans  401Ks, it s himself. How can he make more money, how can he put more money in his bank account? Lewandowski compared the Brexit decision to Trump s rise in the GOP, and Quinn once again called him out and put him back in his place. She said: Trump touted that he saw this coming. That s ridiculous because when he was first asked about Brexit by the press, he didn t appear to know what it was. Lewandowski tried to counter by insisting that People are too smart, they are tired of being told what to do.  He then tried to commend Trump for being a selfish moron: You know what Donald Trump said about Brexit? What he said was, you don t have to listen to me because it s not my decision. He didn t weigh in like Hillary Clinton did, like Barack Obama did, saying that you can t do this. Quinn fought back, Because he didn t know what it was. Lewandowski was fighting a losing battle. Trump s reaction to Brexit was just as terrifying as it was humorous   it truly proved that Trump knows nothing about foreign affairs, and hasn t spent any time educating himself since the beginning of his presidential candidacy. If only some of the hours he spent getting into fights on Twitter were being used for learning about how the world works. But instead, he once again exposed himself as an unfit choice for President. And when people like Lewandowski try to make sense of his idiocy, they only make themselves look equally foolish.You can watch the embarrassing video below:Featured image via screen capture\",\n          \"A Michigan woman decided to defend against tyranny? when she and another shopper couldn t agree over who got to buy the last notebook on the shelf at the Novi Towne Center store.According to ABC 13, the brawl   yes, brawl    involved two Farmington Hills residents, ages 46 and 32, and a mother and daughter from South Lyon, ages 51 and 20. In other words, these were all grown adults who should have known better but hey   there was only one notebook on the shelf, and we ve all seen what happens in those post-apocalyptic movies when a store is down to the last gallon of milk, right?Two of the women, one of whom was the unnamed 20-year-old, reached for the notebook at the same time. The 46 and 32-year-olds apparently decided that she wasn t getting their goddamn notebook and began pulling her hair.Then, because this had almost hit peak  trailer park,  the 20-year-old s mother decided to go for bonus points by pulling out her gun. Fortunately, someone pushed her aside before she could do any harm.This is one of the NRA s  responsible gun owners  (conservatives can t dismiss this one, as it is confirmed that she is a concealed carry permit holder)   ready to leap into action at the most minor sign of danger and make things worse by turning the situation potentially deadly.Watch it happen below:Featured image via screengrab\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"News\",\n          \"politics\",\n          \"Middle-east\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1681,\n        \"samples\": [\n          \"Jun 5, 2015\",\n          \"August 28, 2016\",\n          \"June 3, 2017\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "fake_news_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-618db47f-0afe-436d-8a46-17062a2b0875\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-618db47f-0afe-436d-8a46-17062a2b0875')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-618db47f-0afe-436d-8a46-17062a2b0875 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-618db47f-0afe-436d-8a46-17062a2b0875');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fd1389c8-3fa3-410f-85cd-f7484791a27a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fd1389c8-3fa3-410f-85cd-f7484791a27a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fd1389c8-3fa3-410f-85cd-f7484791a27a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
              "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
              "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
              "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
              "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
              "\n",
              "                                                text subject  \\\n",
              "0  Donald Trump just couldn t wish all Americans ...    News   \n",
              "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
              "2  On Friday, it was revealed that former Milwauk...    News   \n",
              "3  On Christmas day, Donald Trump announced that ...    News   \n",
              "4  Pope Francis used his annual Christmas Day mes...    News   \n",
              "\n",
              "                date  \n",
              "0  December 31, 2017  \n",
              "1  December 31, 2017  \n",
              "2  December 30, 2017  \n",
              "3  December 29, 2017  \n",
              "4  December 25, 2017  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print the contents of the Fake news dataset\n",
        "print(\"\\nFake News Dataset:\")\n",
        "fake_news_df.head()  # Print the first 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wal7lZ6Mjmri"
      },
      "outputs": [],
      "source": [
        "# combining datasets\n",
        "true_news_df['label'] = 1  # Label for true news\n",
        "fake_news_df['label'] = 0   # Label for fake news\n",
        "combined_df = pd.concat([true_news_df, fake_news_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "k6I2VEtjjofu",
        "outputId": "16488d4d-44db-4710-9eeb-734834ef3f84"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"combined_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Trump blames 'both sides' for Virginia violence as many Republicans balk\",\n          \"After primary win, Senate Banking chair may move some nominees\",\n          \"Putin: We'll have to retaliate against 'illegal' U.S. sanctions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"WASHINGTON/NEW YORK (Reuters) - U.S. President Donald Trump inflamed tension after a deadly rally by white nationalists in Virginia by insisting that counter protesters were also to blame, drawing condemnation from some Republican leaders and praise from white supremacists.  In a combative news conference, Trump backed off from his Monday statements explicitly denouncing the Ku Klux Klan, neo-Nazis and white supremacists for the violence that erupted at a \\u201cUnite the Right\\u201d rally in Charlottesville, and reverted to his weekend contention that \\u201cmany sides\\u201d were to blame. \\u201cYou had a group on one side that was bad,\\u201d Trump said on Tuesday. \\u201cAnd you had a group on the other side that was also very violent. And nobody wants to say that. But I\\u2019ll say it right now.\\u201d  Trump later said, \\u201cI think there is blame on both sides and I have no doubt about it,\\u201d adding that there were \\u201cvery fine people\\u201d on both sides. At the weekend rally against the removal of a statue of Robert E. Lee, commander of the pro-slavery Confederate army during the U.S. Civil War, many participants were seen carrying firearms, sticks, shields, and lit torches. Some wore helmets. Counter-protesters came equipped with sticks, helmets and shields. James Fields, a 20-year-old Ohio man who is said to have harbored Nazi sympathies, was charged with murder after the car he was driving plowed into a crowd of counter protesters, killing 32-year-old Heather Heyer on Saturday and injuring 19. A memorial service for Heyer is planned in Charlottesville on Wednesday. Trump\\u2019s remarks drew swift criticism from many Republican leaders. \\u201cNo, not the same,\\u201d former Massachusetts governor and Republican presidential candidate Mitt Romney wrote on Twitter. \\u201cOne side is racist, bigoted, Nazi. The other opposes racism and bigotry. Morally different universes.\\u201d  U.S. Senator Marco Rubio of Florida, who vied with Trump for the Republican presidential nomination, also responded in a series of Twitter posts. \\u201cThe organizers of events which inspired and led to #charlottesvilleterroristattack are 100 percent to blame for a number of reasons,\\u201d Rubio began. \\u201cMr. President, you can\\u2019t allow #WhiteSupremacists to share only part of the blame. They support idea which cost nation and world so much pain \\u2026 the #WhiteSupremacy groups will see being assigned only 50 percent of the blame as a win,\\u201d Rubio added. Former Ku Klux Klan leader David Duke applauded Trump for his \\u201chonesty & courage\\u201d on Twitter. Richard Spencer, the head of a white nationalist group, wrote on Twitter that he was \\u201cproud of him for speaking the truth.\\u201d Richard Trumka, the president of the AFL-CIO labor federation representing 12.5 million workers, resigned from Trump\\u2019s American Manufacturing Council, joining a series of chief executives in doing so. White House officials hoping to put the controversy behind them, worried the conference would revive and intensify the controversy. Asked about next steps, one official said: \\u201cI think next steps are just to stop talking.\\u201d Hours later, the White House sent its regular \\u201cevening communications briefing\\u201d of talking points on the \\u201cnews of the day\\u201d to Republican lawmakers, copies obtained by multiple news organizations, including CNN and the Atlantic, showed. The first summary point read: \\u201cThe President was entirely correct \\u2013 both sides of the violence in Charlottesville acted inappropriately, and bear some responsibility.\\u201d \",\n          \"WASHINGTON (Reuters) - Senate Banking Committee Chairman Richard Shelby, fresh off his primary victory in Alabama, signaled on Wednesday that he may soon end his moratorium on some of the 16 Obama administration financial nominees awaiting panel confirmation. The stalled nominees include those for two Federal Reserve Board governors, two Securities and Exchange commissioners, a U.S. Export-Import Bank board member needed to approve large financing deals and the U.S. Treasury\\u2019s top anti-terrorist finance official.     Banking Committee Democrats and some policy analysts had speculated that Shelby, 81, might be more willing to break the logjam once he defeated a 33-year-old Tea Party-backed primary challenger who had tried to make an issue of his age and long tenure in Washington.  Shelby returned to Washington for a Senate vote on Wednesday afternoon, but Reuters could not reach him in the Capitol. His spokeswoman, Torrie Matous, said that the five-term senator now \\u201cintends to address a number of issues throughout the year including some nominations.\\u201d She added that he would likely begin announcing some hearing plans next week. In Alabama on Tuesday, Shelby easily defeated four challengers to take 65 percent of the vote, enough to avoid a runoff election in April. In a campaign atmosphere dominated by Republican presidential front-runner Donald Trump\\u2019s anti-establishment message, Shelby did the opposite, running on his experience and emphasizing his \\u201cpower base in Washington, D.C.\\u201d  Earlier this week, Democrats on the Banking Committee urged Shelby to \\u201cstop obstructing\\u201d the nominations. One of the most acute problems is for the Export-Import Bank, which cannot approve loans or guarantees above $10 million without Senate confirmation of a third board member \\u2014effectively locking it out of deals for Boeing commercial aircraft or major power equipment made by General Electric Conservatives, including Shelby, had waged a major campaign to close the trade bank last year, idling it for more than five months before Congress voted to renew its charter. The Fed nominees, former community banker Allan Landon and University of Michigan economist Kathryn Dominguez, would restore the central bank\\u2019s board to its full capacity of seven members.  Many had written off their chances of being confirmed, in part because of the testy relationship between Congress and the Fed, which has opposed Republican bills it says would curb its independence. Shelby had previously said that he would not move the 16 nominees until the Obama administration named a Fed vice chair for supervision, a position created by the Dodd-Frank financial reform law. \",\n          \"SAVONLINNA, Finland (Reuters) - President Vladimir Putin said on Thursday that Russia would be forced to retaliate if Washington pressed ahead with what he called illegal new sanctions against Moscow, describing U.S. conduct towards his country as boorish and unreasonable. Putin, speaking on a visit to Finland, was commenting on a vote by the U.S. House of Representatives which on Tuesday decided to impose new sanctions on Moscow and to force President Donald Trump to obtain lawmakers\\u2019 permission before easing any sanctions on Russia.  The sanctions have yet to be approved by the Senate or Trump, and a top White House aide said on Thursday that Trump could veto the legislation in order to push for a tougher deal. Putin, who has repeatedly denied U.S. allegations that Russia interfered with last year\\u2019s U.S. presidential election, said Moscow would only decide on how to retaliate against Washington once it had seen the final text of the proposed law. \\u201cAs you know, we are exercising restraint and patience, but at some moment we\\u2019ll have to retaliate. It\\u2019s impossible to endlessly tolerate this boorishness towards our country,\\u201d Putin told a joint news conference with his Finnish counterpart. \\u201cWhen will our response follow? What will it be? That will depend on the final version of the draft law which is now being debated in the U.S. Senate.\\u201d Putin also spoke about an ongoing diplomatic row between Moscow and Washington which erupted last December when then U.S. President Barack Obama ordered the seizure of Russian diplomatic property in the United States and the expulsion of 35 Russian diplomats. \\u201cThis goes beyond all reasonable bounds,\\u201d said Putin. \\u201cAnd now these sanctions - they are also absolutely unlawful from the point of view of international law.\\u201d Calling the proposed sanctions \\u201cextremely cynical,\\u201d Putin said the demarche looked like an attempt by Washington to use its \\u201cgeopolitical advantages ... to safeguard its economic interests at the expense of its allies\\u201d. He dismissed Congressional investigations into Russia\\u2019s alleged meddling in last year\\u2019s U.S. presidential election, calling them a symptom of growing anti-Russian hysteria in the United States and a result of U.S. domestic politics. \\u201cIt\\u2019s very sad that U.S.-Russian relations are being sacrificed to resolve internal policy issues in the U.S,\\u201d said Putin. \\u201cIt\\u2019s a pity, because acting together we could be solving jointly the most acute problems that worry the peoples of Russia and the United States much more efficiently.\\u201d  However, Putin said that Moscow had \\u201cmany friends\\u201d in the United States and hoped that one day the situation would right itself.  \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"News\",\n          \"politicsNews\",\n          \"Government News\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"August 15, 2017 \",\n          \"March 3, 2016 \",\n          \"July 27, 2017 \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-098cc22f-35ac-483a-aa8b-515e099cf82a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21853</th>\n",
              "      <td>Mother Of DACA Recipient Who Died Rescuing Fl...</td>\n",
              "      <td>Donald Trump is set to end the Deferred Action...</td>\n",
              "      <td>News</td>\n",
              "      <td>September 4, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2139</th>\n",
              "      <td>Trump blames 'both sides' for Virginia violenc...</td>\n",
              "      <td>WASHINGTON/NEW YORK (Reuters) - U.S. President...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>August 15, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>Putin: We'll have to retaliate against 'illega...</td>\n",
              "      <td>SAVONLINNA, Finland (Reuters) - President Vlad...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>July 27, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37377</th>\n",
              "      <td>Keith Olbermann to Betsy DeVos: “The Hurricane...</td>\n",
              "      <td>The lefty lunatic of the day just can t keep h...</td>\n",
              "      <td>Government News</td>\n",
              "      <td>Aug 26, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10554</th>\n",
              "      <td>After primary win, Senate Banking chair may mo...</td>\n",
              "      <td>WASHINGTON (Reuters) - Senate Banking Committe...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>March 3, 2016</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-098cc22f-35ac-483a-aa8b-515e099cf82a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-098cc22f-35ac-483a-aa8b-515e099cf82a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-098cc22f-35ac-483a-aa8b-515e099cf82a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a3604bb6-31fc-48e2-a140-abb43da82b25\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a3604bb6-31fc-48e2-a140-abb43da82b25')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a3604bb6-31fc-48e2-a140-abb43da82b25 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                   title  \\\n",
              "21853   Mother Of DACA Recipient Who Died Rescuing Fl...   \n",
              "2139   Trump blames 'both sides' for Virginia violenc...   \n",
              "2467   Putin: We'll have to retaliate against 'illega...   \n",
              "37377  Keith Olbermann to Betsy DeVos: “The Hurricane...   \n",
              "10554  After primary win, Senate Banking chair may mo...   \n",
              "\n",
              "                                                    text          subject  \\\n",
              "21853  Donald Trump is set to end the Deferred Action...             News   \n",
              "2139   WASHINGTON/NEW YORK (Reuters) - U.S. President...     politicsNews   \n",
              "2467   SAVONLINNA, Finland (Reuters) - President Vlad...     politicsNews   \n",
              "37377  The lefty lunatic of the day just can t keep h...  Government News   \n",
              "10554  WASHINGTON (Reuters) - Senate Banking Committe...     politicsNews   \n",
              "\n",
              "                    date  label  \n",
              "21853  September 4, 2017      0  \n",
              "2139    August 15, 2017       1  \n",
              "2467      July 27, 2017       1  \n",
              "37377       Aug 26, 2017      0  \n",
              "10554     March 3, 2016       1  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cx8vg3w6jqCw"
      },
      "outputs": [],
      "source": [
        "# Define a function to clean the text\n",
        "def clean_text(text):\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the 'text' column\n",
        "combined_df['cleaned_text'] = combined_df['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "7a41sxhKjry0",
        "outputId": "27a6bdae-01c0-4ea9-ea6a-df664e81af63"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'washington reuters head conservative republican faction us congress voted month huge expansion national debt pay tax cuts called fiscal conservative sunday urged budget restraint 2018 keeping sharp pivot way among republicans us representative mark meadows speaking cbs face nation drew hard line federal spending lawmakers bracing battle january return holidays wednesday lawmakers begin trying pass federal budget fight likely linked issues immigration policy even november congressional election campaigns approach republicans seek keep control congress president donald trump republicans want big budget increase military spending democrats also want proportional increases nondefense discretionary spending programs support education scientific research infrastructure public health environmental protection trump administration already willing say going increase nondefense discretionary spending 7 percent meadows chairman small influential house freedom caucus said program democrats saying thats enough need give government pay raise 10 11 percent fiscal conservative dont see rationale eventually run peoples money said meadows among republicans voted late december partys debtfinanced tax overhaul expected balloon federal budget deficit add 15 trillion 10 years 20 trillion national debt interesting hear mark talk fiscal responsibility democratic us representative joseph crowley said cbs crowley said republican tax bill would require united states borrow 15 trillion paid future generations finance tax cuts corporations rich one least fiscally responsible bills weve ever seen passed history house representatives think going paying many many years come crowley said republicans insist tax package biggest us tax overhaul 30 years boost economy job growth house speaker paul ryan also supported tax bill recently went meadows making clear radio interview welfare entitlement reform party often calls would top republican priority 2018 republican parlance entitlement programs mean food stamps housing assistance medicare medicaid health insurance elderly poor disabled well programs created washington assist needy democrats seized ryans early december remarks saying showed republicans would try pay tax overhaul seeking spending cuts social programs goals house republicans may take back seat senate votes democrats needed approve budget prevent government shutdown democrats use leverage senate republicans narrowly control defend discretionary nondefense programs social spending tackling issue dreamers people brought illegally country children trump september put march 2018 expiration date deferred action childhood arrivals daca program protects young immigrants deportation provides work permits president said recent twitter messages wants funding proposed mexican border wall immigration law changes exchange agreeing help dreamers representative debbie dingell told cbs favor linking issue policy objectives wall funding need daca clean said wednesday trump aides meet congressional leaders discuss issues followed weekend strategy sessions trump republican leaders jan 6 7 white house said trump also scheduled meet sunday florida republican governor rick scott wants emergency aid house passed 81 billion aid package hurricanes florida texas puerto rico wildfires california package far exceeded 44 billion requested trump administration senate yet voted aid'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_df['cleaned_text'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement and Evaluate Transformer-Based Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igce34NPqQhM",
        "outputId": "bad961f0-711f-4e67-c8a1-79c9fcefce06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = TFBertModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fkfxD8lyqWCu"
      },
      "outputs": [],
      "source": [
        "# Prepare the data\n",
        "X = combined_df['cleaned_text'].values\n",
        "y = combined_df['label'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ny65YNtEqZv-"
      },
      "outputs": [],
      "source": [
        "# Tokenize and generate embeddings in batches\n",
        "max_length = 128  # Adjust based on your needs\n",
        "batch_size = 8  # Adjust as needed\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "        for text in batch_texts:\n",
        "            encoded_dict = tokenizer.encode_plus(\n",
        "                                text,\n",
        "                                add_special_tokens=True,\n",
        "                                max_length=max_length,\n",
        "                                pad_to_max_length=True,\n",
        "                                return_attention_mask=True,\n",
        "                                return_tensors='tf',\n",
        "                           )\n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "        input_ids = tf.concat(input_ids, axis=0)\n",
        "        attention_masks = tf.concat(attention_masks, axis=0)\n",
        "        embeddings = bert_model(input_ids, attention_mask=attention_masks)[0][:, 0, :]\n",
        "        all_embeddings.append(embeddings)\n",
        "\n",
        "    # Concatenate all batch embeddings\n",
        "    return tf.concat(all_embeddings, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl-NgFA9qc1f",
        "outputId": "7bd0605f-1824-4803-894a-6624b7ffd4d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2673: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_embeddings = generate_embeddings(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "VEhX3k3WsNMn"
      },
      "outputs": [],
      "source": [
        "test_embeddings = generate_embeddings(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "0lkucoHNyP83"
      },
      "outputs": [],
      "source": [
        "# Reshape the embeddings to include a timestep dimension\n",
        "train_embeddings = tf.expand_dims(train_embeddings, axis=1)  # Add timestep dimension\n",
        "test_embeddings = tf.expand_dims(test_embeddings, axis=1)  # Add timestep dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwfIPFR4qz-L",
        "outputId": "c0a74534-e006-4f04-b577-b16032f79534"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9063 - loss: 0.2253 - val_accuracy: 0.9674 - val_loss: 0.0883\n",
            "Epoch 2/3\n",
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9665 - loss: 0.0889 - val_accuracy: 0.9570 - val_loss: 0.1054\n",
            "Epoch 3/3\n",
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9715 - loss: 0.0785 - val_accuracy: 0.9761 - val_loss: 0.0677\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x799db96cbeb0>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build and train the RNN model\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(train_embeddings.shape[1], train_embeddings.shape[2])))  # input_shape is now (timesteps, features)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_embeddings, y_train, epochs=3, batch_size=32, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVJfETuDqkRA",
        "outputId": "4a3fec25-df42-442c-a729-52c3dd0eaf76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9765 - loss: 0.0649\n",
            "Test Loss: 0.0638928934931755\n",
            "Test Accuracy: 0.975278377532959\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_embeddings, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsKf3Q5eyzK3",
        "outputId": "92f7dea8-ba62-4163-aeb1-a940f9df7304"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9046 - loss: 0.2227 - val_accuracy: 0.9325 - val_loss: 0.1678\n",
            "Epoch 2/3\n",
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9641 - loss: 0.0950 - val_accuracy: 0.9635 - val_loss: 0.0976\n",
            "Epoch 3/3\n",
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9697 - loss: 0.0794 - val_accuracy: 0.9737 - val_loss: 0.0697\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x799db393bfd0>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build and train the GRU model\n",
        "gru_model = Sequential()\n",
        "gru_model.add(tf.keras.layers.GRU(64, input_shape=(train_embeddings.shape[1], train_embeddings.shape[2])))\n",
        "gru_model.add(Dense(1, activation='sigmoid'))\n",
        "gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "gru_model.fit(train_embeddings, y_train, epochs=3, batch_size=32, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enP68t0ey2ZU",
        "outputId": "10bc2115-03ec-4899-9e98-761b92661259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9763 - loss: 0.0666\n",
            "GRU Test Loss: 0.06627225875854492\n",
            "GRU Test Accuracy: 0.975612461566925\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the GRU model\n",
        "loss, accuracy = gru_model.evaluate(test_embeddings, y_test)\n",
        "print(f'GRU Test Loss: {loss}')\n",
        "print(f'GRU Test Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9wT5JEbzDUR",
        "outputId": "37d75f14-239c-436f-945b-41a4396d8f37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9110 - loss: 0.2135 - val_accuracy: 0.9605 - val_loss: 0.1022\n",
            "Epoch 2/3\n",
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9659 - loss: 0.0895 - val_accuracy: 0.9688 - val_loss: 0.0818\n",
            "Epoch 3/3\n",
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9709 - loss: 0.0733 - val_accuracy: 0.9747 - val_loss: 0.0684\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x799db3f76650>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build and train the Bi-LSTM model\n",
        "bi_lstm_model = Sequential()\n",
        "bi_lstm_model.add(tf.keras.layers.Bidirectional(LSTM(64), input_shape=(train_embeddings.shape[1], train_embeddings.shape[2])))\n",
        "bi_lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "bi_lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "bi_lstm_model.fit(train_embeddings, y_train, epochs=3, batch_size=32, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMIK_nI2zHB3",
        "outputId": "a0572a68-99ad-48f5-9a72-655cab63c9cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.0629\n",
            "Bi-LSTM Test Loss: 0.06397845596075058\n",
            "Bi-LSTM Test Accuracy: 0.9753897786140442\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the Bi-LSTM model\n",
        "loss, accuracy = bi_lstm_model.evaluate(test_embeddings, y_test)\n",
        "print(f'Bi-LSTM Test Loss: {loss}')\n",
        "print(f'Bi-LSTM Test Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R5FBTLR3zvM4"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz92h1JQzy6x",
        "outputId": "667b47dd-906c-4101-de77-09c8aa35790d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=1) # Binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iICV3X18z3ZL"
      },
      "outputs": [],
      "source": [
        "# Tokenize and prepare data for BERT\n",
        "def prepare_data(texts, labels):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  for text in texts:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "          text,\n",
        "          add_special_tokens=True,\n",
        "          max_length=128,\n",
        "          padding='max_length',\n",
        "          truncation=True,\n",
        "          return_attention_mask=True,\n",
        "          return_tensors='tf'\n",
        "      )\n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0), tf.convert_to_tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5feicSVvz682"
      },
      "outputs": [],
      "source": [
        "train_input_ids, train_attention_masks, train_labels = prepare_data(X_train, y_train)\n",
        "val_input_ids, val_attention_masks, val_labels = prepare_data(X_val, y_val)\n",
        "test_input_ids, test_attention_masks, test_labels = prepare_data(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBUssxAcz9P0",
        "outputId": "0f904758-0457-42eb-9d78-be13bbf07da6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 1, Loss: 0.7131232023239136\n",
            "Epoch 1, Batch 2, Loss: 0.694682776927948\n",
            "Epoch 1, Batch 3, Loss: 0.6841837763786316\n",
            "Epoch 1, Batch 4, Loss: 0.6651768684387207\n",
            "Epoch 1, Batch 5, Loss: 0.6429643630981445\n",
            "Epoch 1, Batch 6, Loss: 0.6498124003410339\n",
            "Epoch 1, Batch 7, Loss: 0.5913978219032288\n",
            "Epoch 1, Batch 8, Loss: 0.5805033445358276\n",
            "Epoch 1, Batch 9, Loss: 0.533775806427002\n",
            "Epoch 1, Batch 10, Loss: 0.5502156019210815\n",
            "Epoch 1, Batch 11, Loss: 0.5049439072608948\n",
            "Epoch 1, Batch 12, Loss: 0.4859670400619507\n",
            "Epoch 1, Batch 13, Loss: 0.46355754137039185\n",
            "Epoch 1, Batch 14, Loss: 0.43990808725357056\n",
            "Epoch 1, Batch 15, Loss: 0.4100261330604553\n",
            "Epoch 1, Batch 16, Loss: 0.4206002354621887\n",
            "Epoch 1, Batch 17, Loss: 0.3789950907230377\n",
            "Epoch 1, Batch 18, Loss: 0.4101824164390564\n",
            "Epoch 1, Batch 19, Loss: 0.3441901206970215\n",
            "Epoch 1, Batch 20, Loss: 0.3613189458847046\n",
            "Epoch 1, Batch 21, Loss: 0.29232674837112427\n",
            "Epoch 1, Batch 22, Loss: 0.3285531997680664\n",
            "Epoch 1, Batch 23, Loss: 0.2478114366531372\n",
            "Epoch 1, Batch 24, Loss: 0.24231970310211182\n",
            "Epoch 1, Batch 25, Loss: 0.3034003973007202\n",
            "Epoch 1, Batch 26, Loss: 0.23716747760772705\n",
            "Epoch 1, Batch 27, Loss: 0.2103481888771057\n",
            "Epoch 1, Batch 28, Loss: 0.19762276113033295\n",
            "Epoch 1, Batch 29, Loss: 0.22404751181602478\n",
            "Epoch 1, Batch 30, Loss: 0.1509665548801422\n",
            "Epoch 1, Batch 31, Loss: 0.14249181747436523\n",
            "Epoch 1, Batch 32, Loss: 0.14673790335655212\n",
            "Epoch 1, Batch 33, Loss: 0.12627282738685608\n",
            "Epoch 1, Batch 34, Loss: 0.11701962351799011\n",
            "Epoch 1, Batch 35, Loss: 0.11174313724040985\n",
            "Epoch 1, Batch 36, Loss: 0.10738502442836761\n",
            "Epoch 1, Batch 37, Loss: 0.09701692312955856\n",
            "Epoch 1, Batch 38, Loss: 0.1022062599658966\n",
            "Epoch 1, Batch 39, Loss: 0.09642563760280609\n",
            "Epoch 1, Batch 40, Loss: 0.1165897473692894\n",
            "Epoch 1, Batch 41, Loss: 0.07551044225692749\n",
            "Epoch 1, Batch 42, Loss: 0.07868054509162903\n",
            "Epoch 1, Batch 43, Loss: 0.06496571749448776\n",
            "Epoch 1, Batch 44, Loss: 0.06307967752218246\n",
            "Epoch 1, Batch 45, Loss: 0.06287769973278046\n",
            "Epoch 1, Batch 46, Loss: 0.054050058126449585\n",
            "Epoch 1, Batch 47, Loss: 0.08893033862113953\n",
            "Epoch 1, Batch 48, Loss: 0.04711040109395981\n",
            "Epoch 1, Batch 49, Loss: 0.04703829810023308\n",
            "Epoch 1, Batch 50, Loss: 0.11515350639820099\n",
            "Epoch 1, Batch 51, Loss: 0.0484466552734375\n",
            "Epoch 1, Batch 52, Loss: 0.03896741941571236\n",
            "Epoch 1, Batch 53, Loss: 0.08894960582256317\n",
            "Epoch 1, Batch 54, Loss: 0.03635641187429428\n",
            "Epoch 1, Batch 55, Loss: 0.03672516345977783\n",
            "Epoch 1, Batch 56, Loss: 0.035603053867816925\n",
            "Epoch 1, Batch 57, Loss: 0.03470534458756447\n",
            "Epoch 1, Batch 58, Loss: 0.03131711483001709\n",
            "Epoch 1, Batch 59, Loss: 0.02834203839302063\n",
            "Epoch 1, Batch 60, Loss: 0.07811520993709564\n",
            "Epoch 1, Batch 61, Loss: 0.04226968437433243\n",
            "Epoch 1, Batch 62, Loss: 0.024340728297829628\n",
            "Epoch 1, Batch 63, Loss: 0.10575960576534271\n",
            "Epoch 1, Batch 64, Loss: 0.024060823023319244\n",
            "Epoch 1, Batch 65, Loss: 0.022323807701468468\n",
            "Epoch 1, Batch 66, Loss: 0.022154372185468674\n",
            "Epoch 1, Batch 67, Loss: 0.11158278584480286\n",
            "Epoch 1, Batch 68, Loss: 0.14290517568588257\n",
            "Epoch 1, Batch 69, Loss: 0.04728482663631439\n",
            "Epoch 1, Batch 70, Loss: 0.01920921914279461\n",
            "Epoch 1, Batch 71, Loss: 0.02085389569401741\n",
            "Epoch 1, Batch 72, Loss: 0.02103520557284355\n",
            "Epoch 1, Batch 73, Loss: 0.01983226090669632\n",
            "Epoch 1, Batch 74, Loss: 0.017606955021619797\n",
            "Epoch 1, Batch 75, Loss: 0.017093660309910774\n",
            "Epoch 1, Batch 76, Loss: 0.02052798680961132\n",
            "Epoch 1, Batch 77, Loss: 0.017443863674998283\n",
            "Epoch 1, Batch 78, Loss: 0.01720733754336834\n",
            "Epoch 1, Batch 79, Loss: 0.016958734020590782\n",
            "Epoch 1, Batch 80, Loss: 0.023199569433927536\n",
            "Epoch 1, Batch 81, Loss: 0.015524264425039291\n",
            "Epoch 1, Batch 82, Loss: 0.015492424368858337\n",
            "Epoch 1, Batch 83, Loss: 0.014099542051553726\n",
            "Epoch 1, Batch 84, Loss: 0.014213191345334053\n",
            "Epoch 1, Batch 85, Loss: 0.013558544218540192\n",
            "Epoch 1, Batch 86, Loss: 0.013397758826613426\n",
            "Epoch 1, Batch 87, Loss: 0.01370413787662983\n",
            "Epoch 1, Batch 88, Loss: 0.013156494125723839\n",
            "Epoch 1, Batch 89, Loss: 0.012907512485980988\n",
            "Epoch 1, Batch 90, Loss: 0.011798064224421978\n",
            "Epoch 1, Batch 91, Loss: 0.011813534423708916\n",
            "Epoch 1, Batch 92, Loss: 0.010925124399363995\n",
            "Epoch 1, Batch 93, Loss: 0.010817335918545723\n",
            "Epoch 1, Batch 94, Loss: 0.011154215782880783\n",
            "Epoch 1, Batch 95, Loss: 0.010493556968867779\n",
            "Epoch 1, Batch 96, Loss: 0.00995183177292347\n",
            "Epoch 1, Batch 97, Loss: 0.009754905477166176\n",
            "Epoch 1, Batch 98, Loss: 0.01599586196243763\n",
            "Epoch 1, Batch 99, Loss: 0.010381590574979782\n",
            "Epoch 1, Batch 100, Loss: 0.013496545143425465\n",
            "Epoch 1, Batch 101, Loss: 0.009641893208026886\n",
            "Epoch 1, Batch 102, Loss: 0.00902421586215496\n",
            "Epoch 1, Batch 103, Loss: 0.010520607233047485\n",
            "Epoch 1, Batch 104, Loss: 0.017377572134137154\n",
            "Epoch 1, Batch 105, Loss: 0.008482862263917923\n",
            "Epoch 1, Batch 106, Loss: 0.009911700151860714\n",
            "Epoch 1, Batch 107, Loss: 0.007821229286491871\n",
            "Epoch 1, Batch 108, Loss: 0.008557596243917942\n",
            "Epoch 1, Batch 109, Loss: 0.008813251741230488\n",
            "Epoch 1, Batch 110, Loss: 0.00822572410106659\n",
            "Epoch 1, Batch 111, Loss: 0.008890427649021149\n",
            "Epoch 1, Batch 112, Loss: 0.007833472453057766\n",
            "Epoch 1, Batch 113, Loss: 0.007666482590138912\n",
            "Epoch 1, Batch 114, Loss: 0.07982316613197327\n",
            "Epoch 1, Batch 115, Loss: 0.007570562418550253\n",
            "Epoch 1, Batch 116, Loss: 0.00688056368380785\n",
            "Epoch 1, Batch 117, Loss: 0.008247149176895618\n",
            "Epoch 1, Batch 118, Loss: 0.006878063082695007\n",
            "Epoch 1, Batch 119, Loss: 0.0071787526831030846\n",
            "Epoch 1, Batch 120, Loss: 0.007119568530470133\n",
            "Epoch 1, Batch 121, Loss: 0.0071504972875118256\n",
            "Epoch 1, Batch 122, Loss: 0.007151536177843809\n",
            "Epoch 1, Batch 123, Loss: 0.006387793924659491\n",
            "Epoch 1, Batch 124, Loss: 0.0065332623198628426\n",
            "Epoch 1, Batch 125, Loss: 0.0061885458417236805\n",
            "Epoch 1, Batch 126, Loss: 0.006039129104465246\n",
            "Epoch 1, Batch 127, Loss: 0.008091248571872711\n",
            "Epoch 1, Batch 128, Loss: 0.08332553505897522\n",
            "Epoch 1, Batch 129, Loss: 0.007578570395708084\n",
            "Epoch 1, Batch 130, Loss: 0.005788350943475962\n",
            "Epoch 1, Batch 131, Loss: 0.006910787895321846\n",
            "Epoch 1, Batch 132, Loss: 0.005562275648117065\n",
            "Epoch 1, Batch 133, Loss: 0.014128629118204117\n",
            "Epoch 1, Batch 134, Loss: 0.006269749253988266\n",
            "Epoch 1, Batch 135, Loss: 0.005459930282086134\n",
            "Epoch 1, Batch 136, Loss: 0.008289612829685211\n",
            "Epoch 1, Batch 137, Loss: 0.0057969121262431145\n",
            "Epoch 1, Batch 138, Loss: 0.006206945050507784\n",
            "Epoch 1, Batch 139, Loss: 0.005468092858791351\n",
            "Epoch 1, Batch 140, Loss: 0.027514148503541946\n",
            "Epoch 1, Batch 141, Loss: 0.02657821960747242\n",
            "Epoch 1, Batch 142, Loss: 0.056137606501579285\n",
            "Epoch 1, Batch 143, Loss: 0.00542413629591465\n",
            "Epoch 1, Batch 144, Loss: 0.00706953089684248\n",
            "Epoch 1, Batch 145, Loss: 0.0057242875918745995\n",
            "Epoch 1, Batch 146, Loss: 0.0060019418597221375\n",
            "Epoch 1, Batch 147, Loss: 0.005493076518177986\n",
            "Epoch 1, Batch 148, Loss: 0.006804256699979305\n",
            "Epoch 1, Batch 149, Loss: 0.006676588673144579\n",
            "Epoch 1, Batch 150, Loss: 0.054825931787490845\n",
            "Epoch 1, Batch 151, Loss: 0.007962466217577457\n",
            "Epoch 1, Batch 152, Loss: 0.09389342367649078\n",
            "Epoch 1, Batch 153, Loss: 0.048454880714416504\n",
            "Epoch 1, Batch 154, Loss: 0.037438374012708664\n",
            "Epoch 1, Batch 155, Loss: 0.06232038512825966\n",
            "Epoch 1, Batch 156, Loss: 0.008610481396317482\n",
            "Epoch 1, Batch 157, Loss: 0.37895750999450684\n",
            "Epoch 1, Batch 158, Loss: 0.006686802953481674\n",
            "Epoch 1, Batch 159, Loss: 0.007424929179251194\n",
            "Epoch 1, Batch 160, Loss: 0.008129813708364964\n",
            "Epoch 1, Batch 161, Loss: 0.008805694058537483\n",
            "Epoch 1, Batch 162, Loss: 0.00906556285917759\n",
            "Epoch 1, Batch 163, Loss: 0.009642057120800018\n",
            "Epoch 1, Batch 164, Loss: 0.01147791463881731\n",
            "Epoch 1, Batch 165, Loss: 0.019618941470980644\n",
            "Epoch 1, Batch 166, Loss: 0.012131795287132263\n",
            "Epoch 1, Batch 167, Loss: 0.009324751794338226\n",
            "Epoch 1, Batch 168, Loss: 0.008668608963489532\n",
            "Epoch 1, Batch 169, Loss: 0.0069383359514176846\n",
            "Epoch 1, Batch 170, Loss: 0.007438174448907375\n",
            "Epoch 1, Batch 171, Loss: 0.007409211713820696\n",
            "Epoch 1, Batch 172, Loss: 0.007168885786086321\n",
            "Epoch 1, Batch 173, Loss: 0.006633225828409195\n",
            "Epoch 1, Batch 174, Loss: 0.009177977219223976\n",
            "Epoch 1, Batch 175, Loss: 0.007706518284976482\n",
            "Epoch 1, Batch 176, Loss: 0.005602001212537289\n",
            "Epoch 1, Batch 177, Loss: 0.04894152283668518\n",
            "Epoch 1, Batch 178, Loss: 0.013825291767716408\n",
            "Epoch 1, Batch 179, Loss: 0.005924222059547901\n",
            "Epoch 1, Batch 180, Loss: 0.006245777476578951\n",
            "Epoch 1, Batch 181, Loss: 0.006353361532092094\n",
            "Epoch 1, Batch 182, Loss: 0.005855664610862732\n",
            "Epoch 1, Batch 183, Loss: 0.0060099950060248375\n",
            "Epoch 1, Batch 184, Loss: 0.012751560658216476\n",
            "Epoch 1, Batch 185, Loss: 0.016691822558641434\n",
            "Epoch 1, Batch 186, Loss: 0.005303733982145786\n",
            "Epoch 1, Batch 187, Loss: 0.006105692125856876\n",
            "Epoch 1, Batch 188, Loss: 0.005917533300817013\n",
            "Epoch 1, Batch 189, Loss: 0.005614799447357655\n",
            "Epoch 1, Batch 190, Loss: 0.029754450544714928\n",
            "Epoch 1, Batch 191, Loss: 0.005837427452206612\n",
            "Epoch 1, Batch 192, Loss: 0.004869035445153713\n",
            "Epoch 1, Batch 193, Loss: 0.005139455199241638\n",
            "Epoch 1, Batch 194, Loss: 0.005850485526025295\n",
            "Epoch 1, Batch 195, Loss: 0.0079960273578763\n",
            "Epoch 1, Batch 196, Loss: 0.05296265706419945\n",
            "Epoch 1, Batch 197, Loss: 0.004646842833608389\n",
            "Epoch 1, Batch 198, Loss: 0.08456801623106003\n",
            "Epoch 1, Batch 199, Loss: 0.005005521234124899\n",
            "Epoch 1, Batch 200, Loss: 0.005858636926859617\n",
            "Epoch 1, Batch 201, Loss: 0.005106868222355843\n",
            "Epoch 1, Batch 202, Loss: 0.030572686344385147\n",
            "Epoch 1, Batch 203, Loss: 0.006688976660370827\n",
            "Epoch 1, Batch 204, Loss: 0.007507039234042168\n",
            "Epoch 1, Batch 205, Loss: 0.00620034895837307\n",
            "Epoch 1, Batch 206, Loss: 0.00707237608730793\n",
            "Epoch 1, Batch 207, Loss: 0.005899003241211176\n",
            "Epoch 1, Batch 208, Loss: 0.005123078357428312\n",
            "Epoch 1, Batch 209, Loss: 0.022352758795022964\n",
            "Epoch 1, Batch 210, Loss: 0.004241606220602989\n",
            "Epoch 1, Batch 211, Loss: 0.0040932935662567616\n",
            "Epoch 1, Batch 212, Loss: 0.007060748524963856\n",
            "Epoch 1, Batch 213, Loss: 0.0036162585020065308\n",
            "Epoch 1, Batch 214, Loss: 0.006569516845047474\n",
            "Epoch 1, Batch 215, Loss: 0.004148046486079693\n",
            "Epoch 1, Batch 216, Loss: 0.0038971740286797285\n",
            "Epoch 1, Batch 217, Loss: 0.0033132564276456833\n",
            "Epoch 1, Batch 218, Loss: 0.0041670300997793674\n",
            "Epoch 1, Batch 219, Loss: 0.004516337998211384\n",
            "Epoch 1, Batch 220, Loss: 0.0041678170673549175\n",
            "Epoch 1, Batch 221, Loss: 0.0034042447805404663\n",
            "Epoch 1, Batch 222, Loss: 0.0038017998449504375\n",
            "Epoch 1, Batch 223, Loss: 0.00402836361899972\n",
            "Epoch 1, Batch 224, Loss: 0.004095283802598715\n",
            "Epoch 1, Batch 225, Loss: 0.0037193684838712215\n",
            "Epoch 1, Batch 226, Loss: 0.0038334233686327934\n",
            "Epoch 1, Batch 227, Loss: 0.0034125284291803837\n",
            "Epoch 1, Batch 228, Loss: 0.0033016190864145756\n",
            "Epoch 1, Batch 229, Loss: 0.0034855592530220747\n",
            "Epoch 1, Batch 230, Loss: 0.00300037139095366\n",
            "Epoch 1, Batch 231, Loss: 0.003597270930185914\n",
            "Epoch 1, Batch 232, Loss: 0.010932868346571922\n",
            "Epoch 1, Batch 233, Loss: 0.0029653513338416815\n",
            "Epoch 1, Batch 234, Loss: 0.002848939271643758\n",
            "Epoch 1, Batch 235, Loss: 0.003038406604900956\n",
            "Epoch 1, Batch 236, Loss: 0.003538738004863262\n",
            "Epoch 1, Batch 237, Loss: 0.0027725151740014553\n",
            "Epoch 1, Batch 238, Loss: 0.002852668985724449\n",
            "Epoch 1, Batch 239, Loss: 0.002773588988929987\n",
            "Epoch 1, Batch 240, Loss: 0.0037567196413874626\n",
            "Epoch 1, Batch 241, Loss: 0.00288024777546525\n",
            "Epoch 1, Batch 242, Loss: 0.005103690549731255\n",
            "Epoch 1, Batch 243, Loss: 0.002584157045930624\n",
            "Epoch 1, Batch 244, Loss: 0.0028999089263379574\n",
            "Epoch 1, Batch 245, Loss: 0.004182138480246067\n",
            "Epoch 1, Batch 246, Loss: 0.0026523505803197622\n",
            "Epoch 1, Batch 247, Loss: 0.002336092758923769\n",
            "Epoch 1, Batch 248, Loss: 0.0026453030295670033\n",
            "Epoch 1, Batch 249, Loss: 0.0034339625854045153\n",
            "Epoch 1, Batch 250, Loss: 0.0025718987453728914\n",
            "Epoch 1, Batch 251, Loss: 0.0030274344608187675\n",
            "Epoch 1, Batch 252, Loss: 0.003566353814676404\n",
            "Epoch 1, Batch 253, Loss: 0.0025511428248137236\n",
            "Epoch 1, Batch 254, Loss: 0.00252761272713542\n",
            "Epoch 1, Batch 255, Loss: 0.0030696727335453033\n",
            "Epoch 1, Batch 256, Loss: 0.0033641548361629248\n",
            "Epoch 1, Batch 257, Loss: 0.00230988347902894\n",
            "Epoch 1, Batch 258, Loss: 0.0022829677909612656\n",
            "Epoch 1, Batch 259, Loss: 0.002940563950687647\n",
            "Epoch 1, Batch 260, Loss: 0.07069125771522522\n",
            "Epoch 1, Batch 261, Loss: 0.002616631332784891\n",
            "Epoch 1, Batch 262, Loss: 0.0023813112638890743\n",
            "Epoch 1, Batch 263, Loss: 0.002185862511396408\n",
            "Epoch 1, Batch 264, Loss: 0.0022514802403748035\n",
            "Epoch 1, Batch 265, Loss: 0.0025610169395804405\n",
            "Epoch 1, Batch 266, Loss: 0.0023700124584138393\n",
            "Epoch 1, Batch 267, Loss: 0.008913682773709297\n",
            "Epoch 1, Batch 268, Loss: 0.002710925415158272\n",
            "Epoch 1, Batch 269, Loss: 0.0027286792173981667\n",
            "Epoch 1, Batch 270, Loss: 0.0023626312613487244\n",
            "Epoch 1, Batch 271, Loss: 0.009258642792701721\n",
            "Epoch 1, Batch 272, Loss: 0.0025245151482522488\n",
            "Epoch 1, Batch 273, Loss: 0.0026507412549108267\n",
            "Epoch 1, Batch 274, Loss: 0.0020228088833391666\n",
            "Epoch 1, Batch 275, Loss: 0.0025491127744317055\n",
            "Epoch 1, Batch 276, Loss: 0.0020829366985708475\n",
            "Epoch 1, Batch 277, Loss: 0.001993827521800995\n",
            "Epoch 1, Batch 278, Loss: 0.0023609260097146034\n",
            "Epoch 1, Batch 279, Loss: 0.00874488428235054\n",
            "Epoch 1, Batch 280, Loss: 0.0020803692750632763\n",
            "Epoch 1, Batch 281, Loss: 0.0020454919431358576\n",
            "Epoch 1, Batch 282, Loss: 0.0020294038113206625\n",
            "Epoch 1, Batch 283, Loss: 0.001997827785089612\n",
            "Epoch 1, Batch 284, Loss: 0.052467335015535355\n",
            "Epoch 1, Batch 285, Loss: 0.002089199610054493\n",
            "Epoch 1, Batch 286, Loss: 0.03413882106542587\n",
            "Epoch 1, Batch 287, Loss: 0.002027809852734208\n",
            "Epoch 1, Batch 288, Loss: 0.002278887201100588\n",
            "Epoch 1, Batch 289, Loss: 0.0021716910414397717\n",
            "Epoch 1, Batch 290, Loss: 0.002279840875416994\n",
            "Epoch 1, Batch 291, Loss: 0.0020798733457922935\n",
            "Epoch 1, Batch 292, Loss: 0.02065534144639969\n",
            "Epoch 1, Batch 293, Loss: 0.0023429994471371174\n",
            "Epoch 1, Batch 294, Loss: 0.002217393135651946\n",
            "Epoch 1, Batch 295, Loss: 0.005961334332823753\n",
            "Epoch 1, Batch 296, Loss: 0.0024583078920841217\n",
            "Epoch 1, Batch 297, Loss: 0.0026178350672125816\n",
            "Epoch 1, Batch 298, Loss: 0.011238995008170605\n",
            "Epoch 1, Batch 299, Loss: 0.002516155829653144\n",
            "Epoch 1, Batch 300, Loss: 0.0021049496717751026\n",
            "Epoch 1, Batch 301, Loss: 0.0023507806472480297\n",
            "Epoch 1, Batch 302, Loss: 0.001990302000194788\n",
            "Epoch 1, Batch 303, Loss: 0.002342600142583251\n",
            "Epoch 1, Batch 304, Loss: 0.0022572434972971678\n",
            "Epoch 1, Batch 305, Loss: 0.005512366537004709\n",
            "Epoch 1, Batch 306, Loss: 0.0021082432940602303\n",
            "Epoch 1, Batch 307, Loss: 0.0019176916684955359\n",
            "Epoch 1, Batch 308, Loss: 0.05757917836308479\n",
            "Epoch 1, Batch 309, Loss: 0.003354802029207349\n",
            "Epoch 1, Batch 310, Loss: 0.001964021474123001\n",
            "Epoch 1, Batch 311, Loss: 0.001716583501547575\n",
            "Epoch 1, Batch 312, Loss: 0.016977110877633095\n",
            "Epoch 1, Batch 313, Loss: 0.001888587255962193\n",
            "Epoch 1, Batch 314, Loss: 0.002253736834973097\n",
            "Epoch 1, Batch 315, Loss: 0.034900058060884476\n",
            "Epoch 1, Batch 316, Loss: 0.00254110898822546\n",
            "Epoch 1, Batch 317, Loss: 0.0031864610500633717\n",
            "Epoch 1, Batch 318, Loss: 0.02538570947945118\n",
            "Epoch 1, Batch 319, Loss: 0.022010641172528267\n",
            "Epoch 1, Batch 320, Loss: 0.004556101281195879\n",
            "Epoch 1, Batch 321, Loss: 0.0037140001077204943\n",
            "Epoch 1, Batch 322, Loss: 0.007261175196617842\n",
            "Epoch 1, Batch 323, Loss: 0.0023334608413279057\n",
            "Epoch 1, Batch 324, Loss: 0.01793435588479042\n",
            "Epoch 1, Batch 325, Loss: 0.0018268413841724396\n",
            "Epoch 1, Batch 326, Loss: 0.0021145811770111322\n",
            "Epoch 1, Batch 327, Loss: 0.003834571223706007\n",
            "Epoch 1, Batch 328, Loss: 0.0017593179363757372\n",
            "Epoch 1, Batch 329, Loss: 0.003988043870776892\n",
            "Epoch 1, Batch 330, Loss: 0.0018625932279974222\n",
            "Epoch 1, Batch 331, Loss: 0.001963813090696931\n",
            "Epoch 1, Batch 332, Loss: 0.001791723887436092\n",
            "Epoch 1, Batch 333, Loss: 0.004486809018999338\n",
            "Epoch 1, Batch 334, Loss: 0.001880074036307633\n",
            "Epoch 1, Batch 335, Loss: 0.0017818750347942114\n",
            "Epoch 1, Batch 336, Loss: 0.001728446688503027\n",
            "Epoch 1, Batch 337, Loss: 0.0022097176406532526\n",
            "Epoch 1, Batch 338, Loss: 0.002154030604287982\n",
            "Epoch 1, Batch 339, Loss: 0.001823105150833726\n",
            "Epoch 1, Batch 340, Loss: 0.001871516346000135\n",
            "Epoch 1, Batch 341, Loss: 0.0018896103138104081\n",
            "Epoch 1, Batch 342, Loss: 0.0018454149831086397\n",
            "Epoch 1, Batch 343, Loss: 0.0016363241011276841\n",
            "Epoch 1, Batch 344, Loss: 0.0015860800631344318\n",
            "Epoch 1, Batch 345, Loss: 0.0018935062689706683\n",
            "Epoch 1, Batch 346, Loss: 0.0016326485201716423\n",
            "Epoch 1, Batch 347, Loss: 0.001582941971719265\n",
            "Epoch 1, Batch 348, Loss: 0.0017165628960356116\n",
            "Epoch 1, Batch 349, Loss: 0.001586854225024581\n",
            "Epoch 1, Batch 350, Loss: 0.0015152276027947664\n",
            "Epoch 1, Batch 351, Loss: 0.0014549766201525927\n",
            "Epoch 1, Batch 352, Loss: 0.0013995488407090306\n",
            "Epoch 1, Batch 353, Loss: 0.0014202925376594067\n",
            "Epoch 1, Batch 354, Loss: 0.0016526848776265979\n",
            "Epoch 1, Batch 355, Loss: 0.001397601212374866\n",
            "Epoch 1, Batch 356, Loss: 0.0014496062649413943\n",
            "Epoch 1, Batch 357, Loss: 0.0013798025902360678\n",
            "Epoch 1, Batch 358, Loss: 0.001532155554741621\n",
            "Epoch 1, Batch 359, Loss: 0.0013956660404801369\n",
            "Epoch 1, Batch 360, Loss: 0.0014408417046070099\n",
            "Epoch 1, Batch 361, Loss: 0.0014667578507214785\n",
            "Epoch 1, Batch 362, Loss: 0.0016314691165462136\n",
            "Epoch 1, Batch 363, Loss: 0.001472559990361333\n",
            "Epoch 1, Batch 364, Loss: 0.0013908667024224997\n",
            "Epoch 1, Batch 365, Loss: 0.0013864876236766577\n",
            "Epoch 1, Batch 366, Loss: 0.0024303884711116552\n",
            "Epoch 1, Batch 367, Loss: 0.0014926912263035774\n",
            "Epoch 1, Batch 368, Loss: 0.001295303227379918\n",
            "Epoch 1, Batch 369, Loss: 0.0013062080834060907\n",
            "Epoch 1, Batch 370, Loss: 0.09064625948667526\n",
            "Epoch 1, Batch 371, Loss: 0.0016225924482569098\n",
            "Epoch 1, Batch 372, Loss: 0.024821456521749496\n",
            "Epoch 1, Batch 373, Loss: 0.0015175212174654007\n",
            "Epoch 1, Batch 374, Loss: 0.001900803647004068\n",
            "Epoch 1, Batch 375, Loss: 0.0017731176922097802\n",
            "Epoch 1, Batch 376, Loss: 0.0016255916561931372\n",
            "Epoch 1, Batch 377, Loss: 0.002339794998988509\n",
            "Epoch 1, Batch 378, Loss: 0.0017434828914701939\n",
            "Epoch 1, Batch 379, Loss: 0.002626278903335333\n",
            "Epoch 1, Batch 380, Loss: 0.0019300798885524273\n",
            "Epoch 1, Batch 381, Loss: 0.0038492558524012566\n",
            "Epoch 1, Batch 382, Loss: 0.0139244319871068\n",
            "Epoch 1, Batch 383, Loss: 0.0020843991078436375\n",
            "Epoch 1, Batch 384, Loss: 0.001641126349568367\n",
            "Epoch 1, Batch 385, Loss: 0.0020755049772560596\n",
            "Epoch 1, Batch 386, Loss: 0.0015399274416267872\n",
            "Epoch 1, Batch 387, Loss: 0.011292489245533943\n",
            "Epoch 1, Batch 388, Loss: 0.0015822022687643766\n",
            "Epoch 1, Batch 389, Loss: 0.0014204052276909351\n",
            "Epoch 1, Batch 390, Loss: 0.001519390381872654\n",
            "Epoch 1, Batch 391, Loss: 0.0015780588146299124\n",
            "Epoch 1, Batch 392, Loss: 0.001313782762736082\n",
            "Epoch 1, Batch 393, Loss: 0.0013666218146681786\n",
            "Epoch 1, Batch 394, Loss: 0.0013058444019407034\n",
            "Epoch 1, Batch 395, Loss: 0.0013342699967324734\n",
            "Epoch 1, Batch 396, Loss: 0.0013257157988846302\n",
            "Epoch 1, Batch 397, Loss: 0.0011969199404120445\n",
            "Epoch 1, Batch 398, Loss: 0.0012565043289214373\n",
            "Epoch 1, Batch 399, Loss: 0.0013042609207332134\n",
            "Epoch 1, Batch 400, Loss: 0.0012072979006916285\n",
            "Epoch 1, Batch 401, Loss: 0.0011959344847127795\n",
            "Epoch 1, Batch 402, Loss: 0.026000037789344788\n",
            "Epoch 1, Batch 403, Loss: 0.0011854434851557016\n",
            "Epoch 1, Batch 404, Loss: 0.0012230097781866789\n",
            "Epoch 1, Batch 405, Loss: 0.08629316091537476\n",
            "Epoch 1, Batch 406, Loss: 0.001448124647140503\n",
            "Epoch 1, Batch 407, Loss: 0.0025442945770919323\n",
            "Epoch 1, Batch 408, Loss: 0.0737956240773201\n",
            "Epoch 1, Batch 409, Loss: 0.008475462906062603\n",
            "Epoch 1, Batch 410, Loss: 0.0019051642157137394\n",
            "Epoch 1, Batch 411, Loss: 0.01689000241458416\n",
            "Epoch 1, Batch 412, Loss: 0.005335761234164238\n",
            "Epoch 1, Batch 413, Loss: 0.0047507393173873425\n",
            "Epoch 1, Batch 414, Loss: 0.0015616838354617357\n",
            "Epoch 1, Batch 415, Loss: 0.001530929235741496\n",
            "Epoch 1, Batch 416, Loss: 0.001624923781491816\n",
            "Epoch 1, Batch 417, Loss: 0.06306620687246323\n",
            "Epoch 1, Batch 418, Loss: 0.001713109901174903\n",
            "Epoch 1, Batch 419, Loss: 0.0027696469333022833\n",
            "Epoch 1, Batch 420, Loss: 0.0019194530323147774\n",
            "Epoch 1, Batch 421, Loss: 0.0018733842298388481\n",
            "Epoch 1, Batch 422, Loss: 0.002061370760202408\n",
            "Epoch 1, Batch 423, Loss: 0.002004266483709216\n",
            "Epoch 1, Batch 424, Loss: 0.005572061985731125\n",
            "Epoch 1, Batch 425, Loss: 0.0025257435627281666\n",
            "Epoch 1, Batch 426, Loss: 0.002131385263055563\n",
            "Epoch 1, Batch 427, Loss: 0.004074154421687126\n",
            "Epoch 1, Batch 428, Loss: 0.0018573501147329807\n",
            "Epoch 1, Batch 429, Loss: 0.001637022476643324\n",
            "Epoch 1, Batch 430, Loss: 0.0021198918111622334\n",
            "Epoch 1, Batch 431, Loss: 0.001629560487344861\n",
            "Epoch 1, Batch 432, Loss: 0.0017348837573081255\n",
            "Epoch 1, Batch 433, Loss: 0.001469526905566454\n",
            "Epoch 1, Batch 434, Loss: 0.001741716405376792\n",
            "Epoch 1, Batch 435, Loss: 0.0017743869684636593\n",
            "Epoch 1, Batch 436, Loss: 0.0022266563028097153\n",
            "Epoch 1, Batch 437, Loss: 0.002515858504921198\n",
            "Epoch 1, Batch 438, Loss: 0.0013362320605665445\n",
            "Epoch 1, Batch 439, Loss: 0.00148325739428401\n",
            "Epoch 1, Batch 440, Loss: 0.001424440648406744\n",
            "Epoch 1, Batch 441, Loss: 0.00391675578430295\n",
            "Epoch 1, Batch 442, Loss: 0.01822226494550705\n",
            "Epoch 1, Batch 443, Loss: 0.001321664429269731\n",
            "Epoch 1, Batch 444, Loss: 0.001200447091832757\n",
            "Epoch 1, Batch 445, Loss: 0.004220384173095226\n",
            "Epoch 1, Batch 446, Loss: 0.0029380887281149626\n",
            "Epoch 1, Batch 447, Loss: 0.0015718321083113551\n",
            "Epoch 1, Batch 448, Loss: 0.0014228482032194734\n",
            "Epoch 1, Batch 449, Loss: 0.10429099202156067\n",
            "Epoch 1, Batch 450, Loss: 0.0013249178882688284\n",
            "Epoch 1, Batch 451, Loss: 0.0014742366038262844\n",
            "Epoch 1, Batch 452, Loss: 0.0031186279375106096\n",
            "Epoch 1, Batch 453, Loss: 0.0015108445659279823\n",
            "Epoch 1, Batch 454, Loss: 0.0014406924601644278\n",
            "Epoch 1, Batch 455, Loss: 0.0013772002421319485\n",
            "Epoch 1, Batch 456, Loss: 0.0016067735850811005\n",
            "Epoch 1, Batch 457, Loss: 0.003996510058641434\n",
            "Epoch 1, Batch 458, Loss: 0.0016013761050999165\n",
            "Epoch 1, Batch 459, Loss: 0.02605951763689518\n",
            "Epoch 1, Batch 460, Loss: 0.0014947117306292057\n",
            "Epoch 1, Batch 461, Loss: 0.0016117247287184\n",
            "Epoch 1, Batch 462, Loss: 0.001518248813226819\n",
            "Epoch 1, Batch 463, Loss: 0.0020604999735951424\n",
            "Epoch 1, Batch 464, Loss: 0.008838153444230556\n",
            "Epoch 1, Batch 465, Loss: 0.0015599405160173774\n",
            "Epoch 1, Batch 466, Loss: 0.0018849296029657125\n",
            "Epoch 1, Batch 467, Loss: 0.0021117818541824818\n",
            "Epoch 1, Batch 468, Loss: 0.04352162033319473\n",
            "Epoch 1, Batch 469, Loss: 0.0031580007635056973\n",
            "Epoch 1, Batch 470, Loss: 0.0012777606025338173\n",
            "Epoch 1, Batch 471, Loss: 0.0014563631266355515\n",
            "Epoch 1, Batch 472, Loss: 0.0015398708637803793\n",
            "Epoch 1, Batch 473, Loss: 0.0013026345986872911\n",
            "Epoch 1, Batch 474, Loss: 0.0014670146629214287\n",
            "Epoch 1, Batch 475, Loss: 0.0019569057039916515\n",
            "Epoch 1, Batch 476, Loss: 0.0011780597269535065\n",
            "Epoch 1, Batch 477, Loss: 0.0012163185747340322\n",
            "Epoch 1, Batch 478, Loss: 0.0020113573409616947\n",
            "Epoch 1, Batch 479, Loss: 0.0012193778529763222\n",
            "Epoch 1, Batch 480, Loss: 0.0012188127730041742\n",
            "Epoch 1, Batch 481, Loss: 0.001138848951086402\n",
            "Epoch 1, Batch 482, Loss: 0.001218416728079319\n",
            "Epoch 1, Batch 483, Loss: 0.0022973669692873955\n",
            "Epoch 1, Batch 484, Loss: 0.0012253141030669212\n",
            "Epoch 1, Batch 485, Loss: 0.001116700703278184\n",
            "Epoch 1, Batch 486, Loss: 0.0012046259362250566\n",
            "Epoch 1, Batch 487, Loss: 0.001197792706079781\n",
            "Epoch 1, Batch 488, Loss: 0.0011680647730827332\n",
            "Epoch 1, Batch 489, Loss: 0.0011596560943871737\n",
            "Epoch 1, Batch 490, Loss: 0.0015212013386189938\n",
            "Epoch 1, Batch 491, Loss: 0.001082494854927063\n",
            "Epoch 1, Batch 492, Loss: 0.0010259925620630383\n",
            "Epoch 2, Batch 1, Loss: 0.09136300534009933\n",
            "Epoch 2, Batch 2, Loss: 0.005570633336901665\n",
            "Epoch 2, Batch 3, Loss: 0.0012450768845155835\n",
            "Epoch 2, Batch 4, Loss: 0.0010502113727852702\n",
            "Epoch 2, Batch 5, Loss: 0.0014130427734926343\n",
            "Epoch 2, Batch 6, Loss: 0.0014629671350121498\n",
            "Epoch 2, Batch 7, Loss: 0.0010303009767085314\n",
            "Epoch 2, Batch 8, Loss: 0.0017239939188584685\n",
            "Epoch 2, Batch 9, Loss: 0.0010449365945532918\n",
            "Epoch 2, Batch 10, Loss: 0.11061099916696548\n",
            "Epoch 2, Batch 11, Loss: 0.0012068117503076792\n",
            "Epoch 2, Batch 12, Loss: 0.0010873805731534958\n",
            "Epoch 2, Batch 13, Loss: 0.0012011302169412374\n",
            "Epoch 2, Batch 14, Loss: 0.0012285977136343718\n",
            "Epoch 2, Batch 15, Loss: 0.0010916833998635411\n",
            "Epoch 2, Batch 16, Loss: 0.0012556309811770916\n",
            "Epoch 2, Batch 17, Loss: 0.008929780684411526\n",
            "Epoch 2, Batch 18, Loss: 0.0013819746673107147\n",
            "Epoch 2, Batch 19, Loss: 0.0012427145848050714\n",
            "Epoch 2, Batch 20, Loss: 0.0013616217765957117\n",
            "Epoch 2, Batch 21, Loss: 0.023609613999724388\n",
            "Epoch 2, Batch 22, Loss: 0.0013332045637071133\n",
            "Epoch 2, Batch 23, Loss: 0.0012566216755658388\n",
            "Epoch 2, Batch 24, Loss: 0.0013843713095411658\n",
            "Epoch 2, Batch 25, Loss: 0.001498987665399909\n",
            "Epoch 2, Batch 26, Loss: 0.0012686196714639664\n",
            "Epoch 2, Batch 27, Loss: 0.001331899082288146\n",
            "Epoch 2, Batch 28, Loss: 0.001224537380039692\n",
            "Epoch 2, Batch 29, Loss: 0.0013702763244509697\n",
            "Epoch 2, Batch 30, Loss: 0.002069122390821576\n",
            "Epoch 2, Batch 31, Loss: 0.0011902921833097935\n",
            "Epoch 2, Batch 32, Loss: 0.0012682460946962237\n",
            "Epoch 2, Batch 33, Loss: 0.0009626558749005198\n",
            "Epoch 2, Batch 34, Loss: 0.0009543629130348563\n",
            "Epoch 2, Batch 35, Loss: 0.0010156943462789059\n",
            "Epoch 2, Batch 36, Loss: 0.0010493138106539845\n",
            "Epoch 2, Batch 37, Loss: 0.016039906069636345\n",
            "Epoch 2, Batch 38, Loss: 0.0011790554272010922\n",
            "Epoch 2, Batch 39, Loss: 0.0009844545274972916\n",
            "Epoch 2, Batch 40, Loss: 0.0015170646365731955\n",
            "Epoch 2, Batch 41, Loss: 0.001382593996822834\n",
            "Epoch 2, Batch 42, Loss: 0.0012641649227589369\n",
            "Epoch 2, Batch 43, Loss: 0.0009720692178234458\n",
            "Epoch 2, Batch 44, Loss: 0.0011679495219141245\n",
            "Epoch 2, Batch 45, Loss: 0.0011373113375157118\n",
            "Epoch 2, Batch 46, Loss: 0.0010516366455703974\n",
            "Epoch 2, Batch 47, Loss: 0.0010222927667200565\n",
            "Epoch 2, Batch 48, Loss: 0.0009551075636409223\n",
            "Epoch 2, Batch 49, Loss: 0.0009592994465492666\n",
            "Epoch 2, Batch 50, Loss: 0.0009965815115720034\n",
            "Epoch 2, Batch 51, Loss: 0.001393417827785015\n",
            "Epoch 2, Batch 52, Loss: 0.0008625453338027\n",
            "Epoch 2, Batch 53, Loss: 0.09981270134449005\n",
            "Epoch 2, Batch 54, Loss: 0.0013054781593382359\n",
            "Epoch 2, Batch 55, Loss: 0.0010550161823630333\n",
            "Epoch 2, Batch 56, Loss: 0.0010747149353846908\n",
            "Epoch 2, Batch 57, Loss: 0.0018444699235260487\n",
            "Epoch 2, Batch 58, Loss: 0.0016269499901682138\n",
            "Epoch 2, Batch 59, Loss: 0.0013854388380423188\n",
            "Epoch 2, Batch 60, Loss: 0.002258110558614135\n",
            "Epoch 2, Batch 61, Loss: 0.00183020974509418\n",
            "Epoch 2, Batch 62, Loss: 0.0013648220337927341\n",
            "Epoch 2, Batch 63, Loss: 0.003302711294963956\n",
            "Epoch 2, Batch 64, Loss: 0.0040745860897004604\n",
            "Epoch 2, Batch 65, Loss: 0.0015852642245590687\n",
            "Epoch 2, Batch 66, Loss: 0.0015405963640660048\n",
            "Epoch 2, Batch 67, Loss: 0.07652989774942398\n",
            "Epoch 2, Batch 68, Loss: 0.0023590822238475084\n",
            "Epoch 2, Batch 69, Loss: 0.002506757155060768\n",
            "Epoch 2, Batch 70, Loss: 0.005316436756402254\n",
            "Epoch 2, Batch 71, Loss: 0.0017747168894857168\n",
            "Epoch 2, Batch 72, Loss: 0.00971192866563797\n",
            "Epoch 2, Batch 73, Loss: 0.001951786456629634\n",
            "Epoch 2, Batch 74, Loss: 0.0015210481360554695\n",
            "Epoch 2, Batch 75, Loss: 0.0014616656117141247\n",
            "Epoch 2, Batch 76, Loss: 0.0037406180053949356\n",
            "Epoch 2, Batch 77, Loss: 0.0021761704701930285\n",
            "Epoch 2, Batch 78, Loss: 0.002117383060976863\n",
            "Epoch 2, Batch 79, Loss: 0.0018714690813794732\n",
            "Epoch 2, Batch 80, Loss: 0.0025845493655651808\n",
            "Epoch 2, Batch 81, Loss: 0.0009927947539836168\n",
            "Epoch 2, Batch 82, Loss: 0.0008985238382592797\n",
            "Epoch 2, Batch 83, Loss: 0.0008944399887695909\n",
            "Epoch 2, Batch 84, Loss: 0.0008809349383227527\n",
            "Epoch 2, Batch 85, Loss: 0.0009441040456295013\n",
            "Epoch 2, Batch 86, Loss: 0.0009193017613142729\n",
            "Epoch 2, Batch 87, Loss: 0.0008593002567067742\n",
            "Epoch 2, Batch 88, Loss: 0.0008436853531748056\n",
            "Epoch 2, Batch 89, Loss: 0.0009497126447968185\n",
            "Epoch 2, Batch 90, Loss: 0.0008120893617160618\n",
            "Epoch 2, Batch 91, Loss: 0.0008841990493237972\n",
            "Epoch 2, Batch 92, Loss: 0.0008161169243976474\n",
            "Epoch 2, Batch 93, Loss: 0.0008212201646529138\n",
            "Epoch 2, Batch 94, Loss: 0.0008478437084704638\n",
            "Epoch 2, Batch 95, Loss: 0.0007895883172750473\n",
            "Epoch 2, Batch 96, Loss: 0.0008048094459809363\n",
            "Epoch 2, Batch 97, Loss: 0.0008087448659352958\n",
            "Epoch 2, Batch 98, Loss: 0.0008822940289974213\n",
            "Epoch 2, Batch 99, Loss: 0.0008071326883509755\n",
            "Epoch 2, Batch 100, Loss: 0.0008815851761028171\n",
            "Epoch 2, Batch 101, Loss: 0.0008401472587138414\n",
            "Epoch 2, Batch 102, Loss: 0.0008533161017112434\n",
            "Epoch 2, Batch 103, Loss: 0.0009031706722453237\n",
            "Epoch 2, Batch 104, Loss: 0.0008499107789248228\n",
            "Epoch 2, Batch 105, Loss: 0.0007746715564280748\n",
            "Epoch 2, Batch 106, Loss: 0.0008297608001157641\n",
            "Epoch 2, Batch 107, Loss: 0.0007234209915623069\n",
            "Epoch 2, Batch 108, Loss: 0.0007727555930614471\n",
            "Epoch 2, Batch 109, Loss: 0.0008898619562387466\n",
            "Epoch 2, Batch 110, Loss: 0.0007749043870717287\n",
            "Epoch 2, Batch 111, Loss: 0.0008293665596283972\n",
            "Epoch 2, Batch 112, Loss: 0.0007715107640251517\n",
            "Epoch 2, Batch 113, Loss: 0.0007874597795307636\n",
            "Epoch 2, Batch 114, Loss: 0.09711086750030518\n",
            "Epoch 2, Batch 115, Loss: 0.0008425763808190823\n",
            "Epoch 2, Batch 116, Loss: 0.00070030870847404\n",
            "Epoch 2, Batch 117, Loss: 0.0012684655375778675\n",
            "Epoch 2, Batch 118, Loss: 0.0007709306082688272\n",
            "Epoch 2, Batch 119, Loss: 0.0007599981618113816\n",
            "Epoch 2, Batch 120, Loss: 0.0008331837598234415\n",
            "Epoch 2, Batch 121, Loss: 0.0007599075324833393\n",
            "Epoch 2, Batch 122, Loss: 0.0007846459047868848\n",
            "Epoch 2, Batch 123, Loss: 0.000787086901254952\n",
            "Epoch 2, Batch 124, Loss: 0.0011283399071544409\n",
            "Epoch 2, Batch 125, Loss: 0.0008341598440892994\n",
            "Epoch 2, Batch 126, Loss: 0.0008469960885122418\n",
            "Epoch 2, Batch 127, Loss: 0.0008718963363207877\n",
            "Epoch 2, Batch 128, Loss: 0.08063647896051407\n",
            "Epoch 2, Batch 129, Loss: 0.0008886698633432388\n",
            "Epoch 2, Batch 130, Loss: 0.0008493646746501327\n",
            "Epoch 2, Batch 131, Loss: 0.0010251300409436226\n",
            "Epoch 2, Batch 132, Loss: 0.000977761810645461\n",
            "Epoch 2, Batch 133, Loss: 0.0011109901824966073\n",
            "Epoch 2, Batch 134, Loss: 0.0011420217342674732\n",
            "Epoch 2, Batch 135, Loss: 0.0012542896438390017\n",
            "Epoch 2, Batch 136, Loss: 0.0016738054109737277\n",
            "Epoch 2, Batch 137, Loss: 0.0012358692474663258\n",
            "Epoch 2, Batch 138, Loss: 0.0023803524672985077\n",
            "Epoch 2, Batch 139, Loss: 0.0024416884407401085\n",
            "Epoch 2, Batch 140, Loss: 0.003303268225863576\n",
            "Epoch 2, Batch 141, Loss: 0.001607875106856227\n",
            "Epoch 2, Batch 142, Loss: 0.016169557347893715\n",
            "Epoch 2, Batch 143, Loss: 0.0015339208766818047\n",
            "Epoch 2, Batch 144, Loss: 0.001258761971257627\n",
            "Epoch 2, Batch 145, Loss: 0.0013579684309661388\n",
            "Epoch 2, Batch 146, Loss: 0.001045851269736886\n",
            "Epoch 2, Batch 147, Loss: 0.0011269575916230679\n",
            "Epoch 2, Batch 148, Loss: 0.014575351029634476\n",
            "Epoch 2, Batch 149, Loss: 0.0010527928825467825\n",
            "Epoch 2, Batch 150, Loss: 0.0009231179137714207\n",
            "Epoch 2, Batch 151, Loss: 0.0011509001487866044\n",
            "Epoch 2, Batch 152, Loss: 0.0009020274737849832\n",
            "Epoch 2, Batch 153, Loss: 0.0011236981954425573\n",
            "Epoch 2, Batch 154, Loss: 0.0010846724035218358\n",
            "Epoch 2, Batch 155, Loss: 0.000989050604403019\n",
            "Epoch 2, Batch 156, Loss: 0.0010384850902482867\n",
            "Epoch 2, Batch 157, Loss: 0.002081742975860834\n",
            "Epoch 2, Batch 158, Loss: 0.0012895631371065974\n",
            "Epoch 2, Batch 159, Loss: 0.000760192982852459\n",
            "Epoch 2, Batch 160, Loss: 0.001141364686191082\n",
            "Epoch 2, Batch 161, Loss: 0.0009541921899653971\n",
            "Epoch 2, Batch 162, Loss: 0.0008213536348193884\n",
            "Epoch 2, Batch 163, Loss: 0.0008743511862121522\n",
            "Epoch 2, Batch 164, Loss: 0.0009437266271561384\n",
            "Epoch 2, Batch 165, Loss: 0.0008437603828497231\n",
            "Epoch 2, Batch 166, Loss: 0.0011644978076219559\n",
            "Epoch 2, Batch 167, Loss: 0.0008992200600914657\n",
            "Epoch 2, Batch 168, Loss: 0.0009807856986299157\n",
            "Epoch 2, Batch 169, Loss: 0.0008341276552528143\n",
            "Epoch 2, Batch 170, Loss: 0.0007916943286545575\n",
            "Epoch 2, Batch 171, Loss: 0.000808196549769491\n",
            "Epoch 2, Batch 172, Loss: 0.002536965534090996\n",
            "Epoch 2, Batch 173, Loss: 0.0008373656310141087\n",
            "Epoch 2, Batch 174, Loss: 0.0010677219834178686\n",
            "Epoch 2, Batch 175, Loss: 0.0007324229227378964\n",
            "Epoch 2, Batch 176, Loss: 0.000743751646950841\n",
            "Epoch 2, Batch 177, Loss: 0.0013548152055591345\n",
            "Epoch 2, Batch 178, Loss: 0.002337409881874919\n",
            "Epoch 2, Batch 179, Loss: 0.005465282127261162\n",
            "Epoch 2, Batch 180, Loss: 0.0007301274454221129\n",
            "Epoch 2, Batch 181, Loss: 0.0007451492128893733\n",
            "Epoch 2, Batch 182, Loss: 0.0007303414167836308\n",
            "Epoch 2, Batch 183, Loss: 0.0007227120222523808\n",
            "Epoch 2, Batch 184, Loss: 0.0007003330392763019\n",
            "Epoch 2, Batch 185, Loss: 0.008129465393722057\n",
            "Epoch 2, Batch 186, Loss: 0.0006620268104597926\n",
            "Epoch 2, Batch 187, Loss: 0.001091883284971118\n",
            "Epoch 2, Batch 188, Loss: 0.0007556707132607698\n",
            "Epoch 2, Batch 189, Loss: 0.0008829023572616279\n",
            "Epoch 2, Batch 190, Loss: 0.0006904323818162084\n",
            "Epoch 2, Batch 191, Loss: 0.0014189612120389938\n",
            "Epoch 2, Batch 192, Loss: 0.0007488371338695288\n",
            "Epoch 2, Batch 193, Loss: 0.0007284020539373159\n",
            "Epoch 2, Batch 194, Loss: 0.0030598952434957027\n",
            "Epoch 2, Batch 195, Loss: 0.0008251300314441323\n",
            "Epoch 2, Batch 196, Loss: 0.0007333059329539537\n",
            "Epoch 2, Batch 197, Loss: 0.0006737681105732918\n",
            "Epoch 2, Batch 198, Loss: 0.11697099357843399\n",
            "Epoch 2, Batch 199, Loss: 0.000721561664249748\n",
            "Epoch 2, Batch 200, Loss: 0.0006425334140658379\n",
            "Epoch 2, Batch 201, Loss: 0.0007026524399407208\n",
            "Epoch 2, Batch 202, Loss: 0.057617537677288055\n",
            "Epoch 2, Batch 203, Loss: 0.0006999353063292801\n",
            "Epoch 2, Batch 204, Loss: 0.000811662757769227\n",
            "Epoch 2, Batch 205, Loss: 0.0007118589128367603\n",
            "Epoch 2, Batch 206, Loss: 0.0008175320690497756\n",
            "Epoch 2, Batch 207, Loss: 0.0007119743386283517\n",
            "Epoch 2, Batch 208, Loss: 0.000683037331327796\n",
            "Epoch 2, Batch 209, Loss: 0.0007931417785584927\n",
            "Epoch 2, Batch 210, Loss: 0.000784100207965821\n",
            "Epoch 2, Batch 211, Loss: 0.0009680952643975616\n",
            "Epoch 2, Batch 212, Loss: 0.011940088123083115\n",
            "Epoch 2, Batch 213, Loss: 0.0007392984116449952\n",
            "Epoch 2, Batch 214, Loss: 0.0011500574182718992\n",
            "Epoch 2, Batch 215, Loss: 0.0012233827728778124\n",
            "Epoch 2, Batch 216, Loss: 0.0018431677017360926\n",
            "Epoch 2, Batch 217, Loss: 0.0013878635363653302\n",
            "Epoch 2, Batch 218, Loss: 0.00676218094304204\n",
            "Epoch 2, Batch 219, Loss: 0.0009145493968389928\n",
            "Epoch 2, Batch 220, Loss: 0.003233798546716571\n",
            "Epoch 2, Batch 221, Loss: 0.0007516287732869387\n",
            "Epoch 2, Batch 222, Loss: 0.0008216672576963902\n",
            "Epoch 2, Batch 223, Loss: 0.0009812535718083382\n",
            "Epoch 2, Batch 224, Loss: 0.0024211322888731956\n",
            "Epoch 2, Batch 225, Loss: 0.03976554796099663\n",
            "Epoch 2, Batch 226, Loss: 0.0008359575876966119\n",
            "Epoch 2, Batch 227, Loss: 0.00072692078538239\n",
            "Epoch 2, Batch 228, Loss: 0.000752394727896899\n",
            "Epoch 2, Batch 229, Loss: 0.0007309488719329238\n",
            "Epoch 2, Batch 230, Loss: 0.0007660246337763965\n",
            "Epoch 2, Batch 231, Loss: 0.0007451343117281795\n",
            "Epoch 2, Batch 232, Loss: 0.04575488716363907\n",
            "Epoch 2, Batch 233, Loss: 0.0007151318714022636\n",
            "Epoch 2, Batch 234, Loss: 0.0006782189593650401\n",
            "Epoch 2, Batch 235, Loss: 0.0007457425817847252\n",
            "Epoch 2, Batch 236, Loss: 0.0019992731977254152\n",
            "Epoch 2, Batch 237, Loss: 0.000684498343616724\n",
            "Epoch 2, Batch 238, Loss: 0.0007143752300180495\n",
            "Epoch 2, Batch 239, Loss: 0.0007089362479746342\n",
            "Epoch 2, Batch 240, Loss: 0.005717230960726738\n",
            "Epoch 2, Batch 241, Loss: 0.0006923473556526005\n",
            "Epoch 2, Batch 242, Loss: 0.0006967130466364324\n",
            "Epoch 2, Batch 243, Loss: 0.0006740709068253636\n",
            "Epoch 2, Batch 244, Loss: 0.0007942906231619418\n",
            "Epoch 2, Batch 245, Loss: 0.0007079982315190136\n",
            "Epoch 2, Batch 246, Loss: 0.0006633647717535496\n",
            "Epoch 2, Batch 247, Loss: 0.000739866285584867\n",
            "Epoch 2, Batch 248, Loss: 0.0006983205094002187\n",
            "Epoch 2, Batch 249, Loss: 0.0010265284217894077\n",
            "Epoch 2, Batch 250, Loss: 0.0006704340921714902\n",
            "Epoch 2, Batch 251, Loss: 0.0006514041451737285\n",
            "Epoch 2, Batch 252, Loss: 0.0007160184904932976\n",
            "Epoch 2, Batch 253, Loss: 0.0006552656996063888\n",
            "Epoch 2, Batch 254, Loss: 0.0007983373943716288\n",
            "Epoch 2, Batch 255, Loss: 0.0008480134420096874\n",
            "Epoch 2, Batch 256, Loss: 0.0007793204858899117\n",
            "Epoch 2, Batch 257, Loss: 0.0006324159912765026\n",
            "Epoch 2, Batch 258, Loss: 0.0006375462980940938\n",
            "Epoch 2, Batch 259, Loss: 0.0007290994981303811\n",
            "Epoch 2, Batch 260, Loss: 0.000768299272749573\n",
            "Epoch 2, Batch 261, Loss: 0.0007194374920800328\n",
            "Epoch 2, Batch 262, Loss: 0.0006434520473703742\n",
            "Epoch 2, Batch 263, Loss: 0.0006201601354405284\n",
            "Epoch 2, Batch 264, Loss: 0.0006100807804614305\n",
            "Epoch 2, Batch 265, Loss: 0.0006349506438709795\n",
            "Epoch 2, Batch 266, Loss: 0.0006368831964209676\n",
            "Epoch 2, Batch 267, Loss: 0.0006624365341849625\n",
            "Epoch 2, Batch 268, Loss: 0.0006083624321036041\n",
            "Epoch 2, Batch 269, Loss: 0.0006582221249118447\n",
            "Epoch 2, Batch 270, Loss: 0.0005759841878898442\n",
            "Epoch 2, Batch 271, Loss: 0.011389810591936111\n",
            "Epoch 2, Batch 272, Loss: 0.0011394501198083162\n",
            "Epoch 2, Batch 273, Loss: 0.0006155294249765575\n",
            "Epoch 2, Batch 274, Loss: 0.0005860638339072466\n",
            "Epoch 2, Batch 275, Loss: 0.0005804786924272776\n",
            "Epoch 2, Batch 276, Loss: 0.0005611500237137079\n",
            "Epoch 2, Batch 277, Loss: 0.0005499649560078979\n",
            "Epoch 2, Batch 278, Loss: 0.0005885469727218151\n",
            "Epoch 2, Batch 279, Loss: 0.0006166384555399418\n",
            "Epoch 2, Batch 280, Loss: 0.0006083854823373258\n",
            "Epoch 2, Batch 281, Loss: 0.0005759838968515396\n",
            "Epoch 2, Batch 282, Loss: 0.0005721233319491148\n",
            "Epoch 2, Batch 283, Loss: 0.0005723065696656704\n",
            "Epoch 2, Batch 284, Loss: 0.0016950028948485851\n",
            "Epoch 2, Batch 285, Loss: 0.0006030422518961132\n",
            "Epoch 2, Batch 286, Loss: 0.003475760342553258\n",
            "Epoch 2, Batch 287, Loss: 0.0005429999437183142\n",
            "Epoch 2, Batch 288, Loss: 0.0005674819694831967\n",
            "Epoch 2, Batch 289, Loss: 0.00054289645049721\n",
            "Epoch 2, Batch 290, Loss: 0.0005483072018250823\n",
            "Epoch 2, Batch 291, Loss: 0.0005252413684502244\n",
            "Epoch 2, Batch 292, Loss: 0.0005557048716582358\n",
            "Epoch 2, Batch 293, Loss: 0.0005670684040524065\n",
            "Epoch 2, Batch 294, Loss: 0.0005549693014472723\n",
            "Epoch 2, Batch 295, Loss: 0.0005268971435725689\n",
            "Epoch 2, Batch 296, Loss: 0.0005256961449049413\n",
            "Epoch 2, Batch 297, Loss: 0.0005898100789636374\n",
            "Epoch 2, Batch 298, Loss: 0.0005363359814509749\n",
            "Epoch 2, Batch 299, Loss: 0.0006089978851377964\n",
            "Epoch 2, Batch 300, Loss: 0.0005266420193947852\n",
            "Epoch 2, Batch 301, Loss: 0.0005793818272650242\n",
            "Epoch 2, Batch 302, Loss: 0.0005492724012583494\n",
            "Epoch 2, Batch 303, Loss: 0.0005658196751028299\n",
            "Epoch 2, Batch 304, Loss: 0.000557730149012059\n",
            "Epoch 2, Batch 305, Loss: 0.0006106399232521653\n",
            "Epoch 2, Batch 306, Loss: 0.0005771442083641887\n",
            "Epoch 2, Batch 307, Loss: 0.0005338059854693711\n",
            "Epoch 2, Batch 308, Loss: 0.0021106156054884195\n",
            "Epoch 2, Batch 309, Loss: 0.0005382626550272107\n",
            "Epoch 2, Batch 310, Loss: 0.0005077221430838108\n",
            "Epoch 2, Batch 311, Loss: 0.0004851726698689163\n",
            "Epoch 2, Batch 312, Loss: 0.0007359776645898819\n",
            "Epoch 2, Batch 313, Loss: 0.0004936081822961569\n",
            "Epoch 2, Batch 314, Loss: 0.0005196647834964097\n",
            "Epoch 2, Batch 315, Loss: 0.001081907656043768\n",
            "Epoch 2, Batch 316, Loss: 0.0005606135819107294\n",
            "Epoch 2, Batch 317, Loss: 0.0004815295687876642\n",
            "Epoch 2, Batch 318, Loss: 0.0005097945686429739\n",
            "Epoch 2, Batch 319, Loss: 0.0007819246384315193\n",
            "Epoch 2, Batch 320, Loss: 0.0005004105623811483\n",
            "Epoch 2, Batch 321, Loss: 0.0005283498321659863\n",
            "Epoch 2, Batch 322, Loss: 0.00047620409168303013\n",
            "Epoch 2, Batch 323, Loss: 0.000533969490788877\n",
            "Epoch 2, Batch 324, Loss: 0.0005267525557428598\n",
            "Epoch 2, Batch 325, Loss: 0.0004900037311017513\n",
            "Epoch 2, Batch 326, Loss: 0.0005130026256665587\n",
            "Epoch 2, Batch 327, Loss: 0.0006871290388517082\n",
            "Epoch 2, Batch 328, Loss: 0.0004757079004775733\n",
            "Epoch 2, Batch 329, Loss: 0.0005043409764766693\n",
            "Epoch 2, Batch 330, Loss: 0.0004917642800137401\n",
            "Epoch 2, Batch 331, Loss: 0.0004904381930828094\n",
            "Epoch 2, Batch 332, Loss: 0.0005565570900216699\n",
            "Epoch 2, Batch 333, Loss: 0.03581957146525383\n",
            "Epoch 2, Batch 334, Loss: 0.0005042902776040137\n",
            "Epoch 2, Batch 335, Loss: 0.00047160766553133726\n",
            "Epoch 2, Batch 336, Loss: 0.0005561055731959641\n",
            "Epoch 2, Batch 337, Loss: 0.0006378625985234976\n",
            "Epoch 2, Batch 338, Loss: 0.0005109573248773813\n",
            "Epoch 2, Batch 339, Loss: 0.000499436107929796\n",
            "Epoch 2, Batch 340, Loss: 0.0005554910167120397\n",
            "Epoch 2, Batch 341, Loss: 0.022982673719525337\n",
            "Epoch 2, Batch 342, Loss: 0.0010371876414865255\n",
            "Epoch 2, Batch 343, Loss: 0.0005907537415623665\n",
            "Epoch 2, Batch 344, Loss: 0.0005317021277733147\n",
            "Epoch 2, Batch 345, Loss: 0.02314518764615059\n",
            "Epoch 2, Batch 346, Loss: 0.0008481890545226634\n",
            "Epoch 2, Batch 347, Loss: 0.0005300829070620239\n",
            "Epoch 2, Batch 348, Loss: 0.0005885089049115777\n",
            "Epoch 2, Batch 349, Loss: 0.0004972204333171248\n",
            "Epoch 2, Batch 350, Loss: 0.0006298713269643486\n",
            "Epoch 2, Batch 351, Loss: 0.000999882584437728\n",
            "Epoch 2, Batch 352, Loss: 0.0005823040846735239\n",
            "Epoch 2, Batch 353, Loss: 0.0008597630658186972\n",
            "Epoch 2, Batch 354, Loss: 0.0005168725620023906\n",
            "Epoch 2, Batch 355, Loss: 0.0005099968984723091\n",
            "Epoch 2, Batch 356, Loss: 0.0004936202894896269\n",
            "Epoch 2, Batch 357, Loss: 0.0004886770620942116\n",
            "Epoch 2, Batch 358, Loss: 0.0005038960371166468\n",
            "Epoch 2, Batch 359, Loss: 0.0004934406606480479\n",
            "Epoch 2, Batch 360, Loss: 0.0006294554332271218\n",
            "Epoch 2, Batch 361, Loss: 0.0023952103219926357\n",
            "Epoch 2, Batch 362, Loss: 0.0004744742764160037\n",
            "Epoch 2, Batch 363, Loss: 0.0005979696870781481\n",
            "Epoch 2, Batch 364, Loss: 0.00046448074863292277\n",
            "Epoch 2, Batch 365, Loss: 0.0006617811159230769\n",
            "Epoch 2, Batch 366, Loss: 0.0004949358408339322\n",
            "Epoch 2, Batch 367, Loss: 0.0005468607414513826\n",
            "Epoch 2, Batch 368, Loss: 0.00046481998288072646\n",
            "Epoch 2, Batch 369, Loss: 0.00046391275827772915\n",
            "Epoch 2, Batch 370, Loss: 0.05102153122425079\n",
            "Epoch 2, Batch 371, Loss: 0.0005153391975909472\n",
            "Epoch 2, Batch 372, Loss: 0.008942624554038048\n",
            "Epoch 2, Batch 373, Loss: 0.0005421040114015341\n",
            "Epoch 2, Batch 374, Loss: 0.0018971029203385115\n",
            "Epoch 2, Batch 375, Loss: 0.0005576781695708632\n",
            "Epoch 2, Batch 376, Loss: 0.0005087723839096725\n",
            "Epoch 2, Batch 377, Loss: 0.0006292794714681804\n",
            "Epoch 2, Batch 378, Loss: 0.0006746058352291584\n",
            "Epoch 2, Batch 379, Loss: 0.0012668900890275836\n",
            "Epoch 2, Batch 380, Loss: 0.0005569622735492885\n",
            "Epoch 2, Batch 381, Loss: 0.0008488322491757572\n",
            "Epoch 2, Batch 382, Loss: 0.0007011092966422439\n",
            "Epoch 2, Batch 383, Loss: 0.0006663290550932288\n",
            "Epoch 2, Batch 384, Loss: 0.0007615481736138463\n",
            "Epoch 2, Batch 385, Loss: 0.002987791085615754\n",
            "Epoch 2, Batch 386, Loss: 0.00068526720860973\n",
            "Epoch 2, Batch 387, Loss: 0.0008536544628441334\n",
            "Epoch 2, Batch 388, Loss: 0.0009878282435238361\n",
            "Epoch 2, Batch 389, Loss: 0.0005386181874200702\n",
            "Epoch 2, Batch 390, Loss: 0.0008125518215820193\n",
            "Epoch 2, Batch 391, Loss: 0.0006490884115919471\n",
            "Epoch 2, Batch 392, Loss: 0.02507157064974308\n",
            "Epoch 2, Batch 393, Loss: 0.0010874222498387098\n",
            "Epoch 2, Batch 394, Loss: 0.000444683333626017\n",
            "Epoch 2, Batch 395, Loss: 0.000550783530343324\n",
            "Epoch 2, Batch 396, Loss: 0.0005229301750659943\n",
            "Epoch 2, Batch 397, Loss: 0.0005369130522012711\n",
            "Epoch 2, Batch 398, Loss: 0.0006568734534084797\n",
            "Epoch 2, Batch 399, Loss: 0.0004945024847984314\n",
            "Epoch 2, Batch 400, Loss: 0.0005453132325783372\n",
            "Epoch 2, Batch 401, Loss: 0.0004715205868706107\n",
            "Epoch 2, Batch 402, Loss: 0.00044112312025390565\n",
            "Epoch 2, Batch 403, Loss: 0.00043545837979763746\n",
            "Epoch 2, Batch 404, Loss: 0.000451073661679402\n",
            "Epoch 2, Batch 405, Loss: 0.00045175303239375353\n",
            "Epoch 2, Batch 406, Loss: 0.00044436525786295533\n",
            "Epoch 2, Batch 407, Loss: 0.0004762214666698128\n",
            "Epoch 2, Batch 408, Loss: 0.0004550885932985693\n",
            "Epoch 2, Batch 409, Loss: 0.0006745537975803018\n",
            "Epoch 2, Batch 410, Loss: 0.00044842567876912653\n",
            "Epoch 2, Batch 411, Loss: 0.0004648109897971153\n",
            "Epoch 2, Batch 412, Loss: 0.0008711341070011258\n",
            "Epoch 2, Batch 413, Loss: 0.0005121974973008037\n",
            "Epoch 2, Batch 414, Loss: 0.00046865514013916254\n",
            "Epoch 2, Batch 415, Loss: 0.00045030409819446504\n",
            "Epoch 2, Batch 416, Loss: 0.0004490783903747797\n",
            "Epoch 2, Batch 417, Loss: 0.00047708023339509964\n",
            "Epoch 2, Batch 418, Loss: 0.000502643350046128\n",
            "Epoch 2, Batch 419, Loss: 0.00046423193998634815\n",
            "Epoch 2, Batch 420, Loss: 0.0004369017551653087\n",
            "Epoch 2, Batch 421, Loss: 0.0004079935606569052\n",
            "Epoch 2, Batch 422, Loss: 0.000506386742927134\n",
            "Epoch 2, Batch 423, Loss: 0.0004705282917711884\n",
            "Epoch 2, Batch 424, Loss: 0.0004681070277001709\n",
            "Epoch 2, Batch 425, Loss: 0.0004268586926627904\n",
            "Epoch 2, Batch 426, Loss: 0.0004204722063150257\n",
            "Epoch 2, Batch 427, Loss: 0.059959299862384796\n",
            "Epoch 2, Batch 428, Loss: 0.0004564349364954978\n",
            "Epoch 2, Batch 429, Loss: 0.0005094772786833346\n",
            "Epoch 2, Batch 430, Loss: 0.0019995507318526506\n",
            "Epoch 2, Batch 431, Loss: 0.04199264943599701\n",
            "Epoch 2, Batch 432, Loss: 0.004962259903550148\n",
            "Epoch 2, Batch 433, Loss: 0.0011631884844973683\n",
            "Epoch 2, Batch 434, Loss: 0.0010494844755157828\n",
            "Epoch 2, Batch 435, Loss: 0.000651245703920722\n",
            "Epoch 2, Batch 436, Loss: 0.0006831261562183499\n",
            "Epoch 2, Batch 437, Loss: 0.0007473740843124688\n",
            "Epoch 2, Batch 438, Loss: 0.0006617710459977388\n",
            "Epoch 2, Batch 439, Loss: 0.0009279447258450091\n",
            "Epoch 2, Batch 440, Loss: 0.0012717284262180328\n",
            "Epoch 2, Batch 441, Loss: 0.000725087127648294\n",
            "Epoch 2, Batch 442, Loss: 0.001984210452064872\n",
            "Epoch 2, Batch 443, Loss: 0.0016291008796542883\n",
            "Epoch 2, Batch 444, Loss: 0.0005076693487353623\n",
            "Epoch 2, Batch 445, Loss: 0.0009167179232463241\n",
            "Epoch 2, Batch 446, Loss: 0.0007561225211247802\n",
            "Epoch 2, Batch 447, Loss: 0.0012618389446288347\n",
            "Epoch 2, Batch 448, Loss: 0.0011660056188702583\n",
            "Epoch 2, Batch 449, Loss: 0.07697862386703491\n",
            "Epoch 2, Batch 450, Loss: 0.0016666421433910728\n",
            "Epoch 2, Batch 451, Loss: 0.002846670337021351\n",
            "Epoch 2, Batch 452, Loss: 0.002424153033643961\n",
            "Epoch 2, Batch 453, Loss: 0.007328180130571127\n",
            "Epoch 2, Batch 454, Loss: 0.010019555687904358\n",
            "Epoch 2, Batch 455, Loss: 0.001725553534924984\n",
            "Epoch 2, Batch 456, Loss: 0.0012085861526429653\n",
            "Epoch 2, Batch 457, Loss: 0.006524824071675539\n",
            "Epoch 2, Batch 458, Loss: 0.005438694264739752\n",
            "Epoch 2, Batch 459, Loss: 0.0020490731112658978\n",
            "Epoch 2, Batch 460, Loss: 0.0005598721909336746\n",
            "Epoch 2, Batch 461, Loss: 0.0005724476650357246\n",
            "Epoch 2, Batch 462, Loss: 0.0005779986968263984\n",
            "Epoch 2, Batch 463, Loss: 0.0006707155844196677\n",
            "Epoch 2, Batch 464, Loss: 0.0005785790272057056\n",
            "Epoch 2, Batch 465, Loss: 0.0006988746463321149\n",
            "Epoch 2, Batch 466, Loss: 0.0006243116222321987\n",
            "Epoch 2, Batch 467, Loss: 0.0018064015312120318\n",
            "Epoch 2, Batch 468, Loss: 0.0005470879259519279\n",
            "Epoch 2, Batch 469, Loss: 0.0005573336384259164\n",
            "Epoch 2, Batch 470, Loss: 0.0005006096325814724\n",
            "Epoch 2, Batch 471, Loss: 0.0005692571285180748\n",
            "Epoch 2, Batch 472, Loss: 0.0006736348732374609\n",
            "Epoch 2, Batch 473, Loss: 0.0005555738462135196\n",
            "Epoch 2, Batch 474, Loss: 0.000577571103349328\n",
            "Epoch 2, Batch 475, Loss: 0.0015492025995627046\n",
            "Epoch 2, Batch 476, Loss: 0.0005203416221775115\n",
            "Epoch 2, Batch 477, Loss: 0.0005467775044962764\n",
            "Epoch 2, Batch 478, Loss: 0.0005280111217871308\n",
            "Epoch 2, Batch 479, Loss: 0.0005250779213383794\n",
            "Epoch 2, Batch 480, Loss: 0.0005379797075875103\n",
            "Epoch 2, Batch 481, Loss: 0.0005076044471934438\n",
            "Epoch 2, Batch 482, Loss: 0.0005500101251527667\n",
            "Epoch 2, Batch 483, Loss: 0.0395914688706398\n",
            "Epoch 2, Batch 484, Loss: 0.0006191771244630218\n",
            "Epoch 2, Batch 485, Loss: 0.0004948294954374433\n",
            "Epoch 2, Batch 486, Loss: 0.0005226549692451954\n",
            "Epoch 2, Batch 487, Loss: 0.000744940247386694\n",
            "Epoch 2, Batch 488, Loss: 0.0005005245911888778\n",
            "Epoch 2, Batch 489, Loss: 0.003317041089758277\n",
            "Epoch 2, Batch 490, Loss: 0.0010249924380332232\n",
            "Epoch 2, Batch 491, Loss: 0.0005673655541613698\n",
            "Epoch 2, Batch 492, Loss: 0.00047491121222265065\n",
            "Epoch 3, Batch 1, Loss: 0.10047420114278793\n",
            "Epoch 3, Batch 2, Loss: 0.00055016262922436\n",
            "Epoch 3, Batch 3, Loss: 0.0057388111017644405\n",
            "Epoch 3, Batch 4, Loss: 0.000613871612586081\n",
            "Epoch 3, Batch 5, Loss: 0.0006329451571218669\n",
            "Epoch 3, Batch 6, Loss: 0.000522213289514184\n",
            "Epoch 3, Batch 7, Loss: 0.007573428098112345\n",
            "Epoch 3, Batch 8, Loss: 0.000816618965473026\n",
            "Epoch 3, Batch 9, Loss: 0.0005973440129309893\n",
            "Epoch 3, Batch 10, Loss: 0.21399648487567902\n",
            "Epoch 3, Batch 11, Loss: 0.0005575107061304152\n",
            "Epoch 3, Batch 12, Loss: 0.0005922113778069615\n",
            "Epoch 3, Batch 13, Loss: 0.0009784752037376165\n",
            "Epoch 3, Batch 14, Loss: 0.000620346749201417\n",
            "Epoch 3, Batch 15, Loss: 0.000660017307382077\n",
            "Epoch 3, Batch 16, Loss: 0.003532310714945197\n",
            "Epoch 3, Batch 17, Loss: 0.0061941673047840595\n",
            "Epoch 3, Batch 18, Loss: 0.0014696907019242644\n",
            "Epoch 3, Batch 19, Loss: 0.0011807401897385716\n",
            "Epoch 3, Batch 20, Loss: 0.002328693401068449\n",
            "Epoch 3, Batch 21, Loss: 0.001281444332562387\n",
            "Epoch 3, Batch 22, Loss: 0.0037928116507828236\n",
            "Epoch 3, Batch 23, Loss: 0.0012681381776928902\n",
            "Epoch 3, Batch 24, Loss: 0.0036332476884126663\n",
            "Epoch 3, Batch 25, Loss: 0.006458892021328211\n",
            "Epoch 3, Batch 26, Loss: 0.0011430957820266485\n",
            "Epoch 3, Batch 27, Loss: 0.0013309273635968566\n",
            "Epoch 3, Batch 28, Loss: 0.0012605811934918165\n",
            "Epoch 3, Batch 29, Loss: 0.0009642416262067854\n",
            "Epoch 3, Batch 30, Loss: 0.0008171868976205587\n",
            "Epoch 3, Batch 31, Loss: 0.000726643018424511\n",
            "Epoch 3, Batch 32, Loss: 0.0007911667926236987\n",
            "Epoch 3, Batch 33, Loss: 0.0005774109158664942\n",
            "Epoch 3, Batch 34, Loss: 0.0006353657227009535\n",
            "Epoch 3, Batch 35, Loss: 0.0007441385532729328\n",
            "Epoch 3, Batch 36, Loss: 0.0005965540185570717\n",
            "Epoch 3, Batch 37, Loss: 0.0005518992547877133\n",
            "Epoch 3, Batch 38, Loss: 0.0006774990470148623\n",
            "Epoch 3, Batch 39, Loss: 0.0005464275600388646\n",
            "Epoch 3, Batch 40, Loss: 0.0007287148619070649\n",
            "Epoch 3, Batch 41, Loss: 0.0005486222798936069\n",
            "Epoch 3, Batch 42, Loss: 0.0007240999257192016\n",
            "Epoch 3, Batch 43, Loss: 0.0005357133923098445\n",
            "Epoch 3, Batch 44, Loss: 0.000563247362151742\n",
            "Epoch 3, Batch 45, Loss: 0.0005186307826079428\n",
            "Epoch 3, Batch 46, Loss: 0.0004697694384958595\n",
            "Epoch 3, Batch 47, Loss: 0.000524738512467593\n",
            "Epoch 3, Batch 48, Loss: 0.0005475507932715118\n",
            "Epoch 3, Batch 49, Loss: 0.0004593599296640605\n",
            "Epoch 3, Batch 50, Loss: 0.0008188164792954922\n",
            "Epoch 3, Batch 51, Loss: 0.0006489362567663193\n",
            "Epoch 3, Batch 52, Loss: 0.0004440493357833475\n",
            "Epoch 3, Batch 53, Loss: 0.0735403448343277\n",
            "Epoch 3, Batch 54, Loss: 0.0005856578936800361\n",
            "Epoch 3, Batch 55, Loss: 0.0005555888637900352\n",
            "Epoch 3, Batch 56, Loss: 0.0006313193589448929\n",
            "Epoch 3, Batch 57, Loss: 0.000921024417039007\n",
            "Epoch 3, Batch 58, Loss: 0.0007599471136927605\n",
            "Epoch 3, Batch 59, Loss: 0.0009752359474077821\n",
            "Epoch 3, Batch 60, Loss: 0.0009147273958660662\n",
            "Epoch 3, Batch 61, Loss: 0.0019789449870586395\n",
            "Epoch 3, Batch 62, Loss: 0.0011950975749641657\n",
            "Epoch 3, Batch 63, Loss: 0.005456908605992794\n",
            "Epoch 3, Batch 64, Loss: 0.0013841015752404928\n",
            "Epoch 3, Batch 65, Loss: 0.0018490657676011324\n",
            "Epoch 3, Batch 66, Loss: 0.002936786040663719\n",
            "Epoch 3, Batch 67, Loss: 0.011593637056648731\n",
            "Epoch 3, Batch 68, Loss: 0.0012384657748043537\n",
            "Epoch 3, Batch 69, Loss: 0.003182305721566081\n",
            "Epoch 3, Batch 70, Loss: 0.0006555764121003449\n",
            "Epoch 3, Batch 71, Loss: 0.0006797517999075353\n",
            "Epoch 3, Batch 72, Loss: 0.0018351685721427202\n",
            "Epoch 3, Batch 73, Loss: 0.0005708048120141029\n",
            "Epoch 3, Batch 74, Loss: 0.0007129482692107558\n",
            "Epoch 3, Batch 75, Loss: 0.00045948560000397265\n",
            "Epoch 3, Batch 76, Loss: 0.0005004371632821858\n",
            "Epoch 3, Batch 77, Loss: 0.0005424064001999795\n",
            "Epoch 3, Batch 78, Loss: 0.0004336722777225077\n",
            "Epoch 3, Batch 79, Loss: 0.0010870120022445917\n",
            "Epoch 3, Batch 80, Loss: 0.000954996794462204\n",
            "Epoch 3, Batch 81, Loss: 0.0004441839992068708\n",
            "Epoch 3, Batch 82, Loss: 0.0004552918253466487\n",
            "Epoch 3, Batch 83, Loss: 0.0004207825695630163\n",
            "Epoch 3, Batch 84, Loss: 0.0006006896146573126\n",
            "Epoch 3, Batch 85, Loss: 0.0006050463998690248\n",
            "Epoch 3, Batch 86, Loss: 0.0004656514502130449\n",
            "Epoch 3, Batch 87, Loss: 0.0004931788425892591\n",
            "Epoch 3, Batch 88, Loss: 0.0004192384076304734\n",
            "Epoch 3, Batch 89, Loss: 0.0005736302118748426\n",
            "Epoch 3, Batch 90, Loss: 0.00041544297710061073\n",
            "Epoch 3, Batch 91, Loss: 0.0004350782837718725\n",
            "Epoch 3, Batch 92, Loss: 0.00039877768722362816\n",
            "Epoch 3, Batch 93, Loss: 0.0004216287052258849\n",
            "Epoch 3, Batch 94, Loss: 0.00039708527037873864\n",
            "Epoch 3, Batch 95, Loss: 0.0004098066419828683\n",
            "Epoch 3, Batch 96, Loss: 0.000493367959279567\n",
            "Epoch 3, Batch 97, Loss: 0.00046399739221669734\n",
            "Epoch 3, Batch 98, Loss: 0.0004573975456878543\n",
            "Epoch 3, Batch 99, Loss: 0.0022246709559112787\n",
            "Epoch 3, Batch 100, Loss: 0.00041868144762702286\n",
            "Epoch 3, Batch 101, Loss: 0.00041104998672381043\n",
            "Epoch 3, Batch 102, Loss: 0.00043243664549663663\n",
            "Epoch 3, Batch 103, Loss: 0.000523052760399878\n",
            "Epoch 3, Batch 104, Loss: 0.00041710856021381915\n",
            "Epoch 3, Batch 105, Loss: 0.00040429417276754975\n",
            "Epoch 3, Batch 106, Loss: 0.000401862314902246\n",
            "Epoch 3, Batch 107, Loss: 0.00042918705730699003\n",
            "Epoch 3, Batch 108, Loss: 0.0004360333550721407\n",
            "Epoch 3, Batch 109, Loss: 0.0015877560945227742\n",
            "Epoch 3, Batch 110, Loss: 0.0004224469303153455\n",
            "Epoch 3, Batch 111, Loss: 0.00043978949543088675\n",
            "Epoch 3, Batch 112, Loss: 0.00040121618076227605\n",
            "Epoch 3, Batch 113, Loss: 0.0003787264577113092\n",
            "Epoch 3, Batch 114, Loss: 0.007595376577228308\n",
            "Epoch 3, Batch 115, Loss: 0.000425924634328112\n",
            "Epoch 3, Batch 116, Loss: 0.0003670097794383764\n",
            "Epoch 3, Batch 117, Loss: 0.0005310423439368606\n",
            "Epoch 3, Batch 118, Loss: 0.0003611835418269038\n",
            "Epoch 3, Batch 119, Loss: 0.0004059705534018576\n",
            "Epoch 3, Batch 120, Loss: 0.00044682793668471277\n",
            "Epoch 3, Batch 121, Loss: 0.00035681124427355826\n",
            "Epoch 3, Batch 122, Loss: 0.0003882745513692498\n",
            "Epoch 3, Batch 123, Loss: 0.0003810759517364204\n",
            "Epoch 3, Batch 124, Loss: 0.00039602757897228\n",
            "Epoch 3, Batch 125, Loss: 0.00038360198959708214\n",
            "Epoch 3, Batch 126, Loss: 0.00036738638300448656\n",
            "Epoch 3, Batch 127, Loss: 0.0006459142314270139\n",
            "Epoch 3, Batch 128, Loss: 0.0004241902497597039\n",
            "Epoch 3, Batch 129, Loss: 0.00039413123158738017\n",
            "Epoch 3, Batch 130, Loss: 0.0003248598659411073\n",
            "Epoch 3, Batch 131, Loss: 0.00038164498982951045\n",
            "Epoch 3, Batch 132, Loss: 0.0003429155913181603\n",
            "Epoch 3, Batch 133, Loss: 0.00040388040360994637\n",
            "Epoch 3, Batch 134, Loss: 0.00037037371657788754\n",
            "Epoch 3, Batch 135, Loss: 0.00034993415465578437\n",
            "Epoch 3, Batch 136, Loss: 0.0008842906099744141\n",
            "Epoch 3, Batch 137, Loss: 0.0003780785482376814\n",
            "Epoch 3, Batch 138, Loss: 0.00039810681482777\n",
            "Epoch 3, Batch 139, Loss: 0.0003619139897637069\n",
            "Epoch 3, Batch 140, Loss: 0.00037774851080030203\n",
            "Epoch 3, Batch 141, Loss: 0.0006881230510771275\n",
            "Epoch 3, Batch 142, Loss: 0.00037120733759365976\n",
            "Epoch 3, Batch 143, Loss: 0.00033447984606027603\n",
            "Epoch 3, Batch 144, Loss: 0.00039125318289734423\n",
            "Epoch 3, Batch 145, Loss: 0.0003553839342202991\n",
            "Epoch 3, Batch 146, Loss: 0.0003427620104048401\n",
            "Epoch 3, Batch 147, Loss: 0.0006964989006519318\n",
            "Epoch 3, Batch 148, Loss: 0.00031696335645392537\n",
            "Epoch 3, Batch 149, Loss: 0.00043095924775116146\n",
            "Epoch 3, Batch 150, Loss: 0.0004106777487322688\n",
            "Epoch 3, Batch 151, Loss: 0.0003223225357942283\n",
            "Epoch 3, Batch 152, Loss: 0.00033103549503721297\n",
            "Epoch 3, Batch 153, Loss: 0.00042029586620628834\n",
            "Epoch 3, Batch 154, Loss: 0.00032730645034462214\n",
            "Epoch 3, Batch 155, Loss: 0.00034283671993762255\n",
            "Epoch 3, Batch 156, Loss: 0.00032688333885744214\n",
            "Epoch 3, Batch 157, Loss: 0.00036349950823932886\n",
            "Epoch 3, Batch 158, Loss: 0.000345314183505252\n",
            "Epoch 3, Batch 159, Loss: 0.0003180982603225857\n",
            "Epoch 3, Batch 160, Loss: 0.0005488009774126112\n",
            "Epoch 3, Batch 161, Loss: 0.00033443342545069754\n",
            "Epoch 3, Batch 162, Loss: 0.0003277704818174243\n",
            "Epoch 3, Batch 163, Loss: 0.000326061446685344\n",
            "Epoch 3, Batch 164, Loss: 0.0003890299121849239\n",
            "Epoch 3, Batch 165, Loss: 0.0003298177616670728\n",
            "Epoch 3, Batch 166, Loss: 0.00033942522713914514\n",
            "Epoch 3, Batch 167, Loss: 0.0003526871441863477\n",
            "Epoch 3, Batch 168, Loss: 0.0004119794466532767\n",
            "Epoch 3, Batch 169, Loss: 0.00035814056172966957\n",
            "Epoch 3, Batch 170, Loss: 0.0003471550589893013\n",
            "Epoch 3, Batch 171, Loss: 0.00030883285216987133\n",
            "Epoch 3, Batch 172, Loss: 0.0003080457681789994\n",
            "Epoch 3, Batch 173, Loss: 0.0003352711792103946\n",
            "Epoch 3, Batch 174, Loss: 0.0003218269266653806\n",
            "Epoch 3, Batch 175, Loss: 0.0003382214636076242\n",
            "Epoch 3, Batch 176, Loss: 0.0004716094408649951\n",
            "Epoch 3, Batch 177, Loss: 0.0008686974178999662\n",
            "Epoch 3, Batch 178, Loss: 0.0002864473790396005\n",
            "Epoch 3, Batch 179, Loss: 0.00033423962304368615\n",
            "Epoch 3, Batch 180, Loss: 0.0003147005336359143\n",
            "Epoch 3, Batch 181, Loss: 0.00032359478063881397\n",
            "Epoch 3, Batch 182, Loss: 0.0003019995056092739\n",
            "Epoch 3, Batch 183, Loss: 0.0003205279354006052\n",
            "Epoch 3, Batch 184, Loss: 0.0003215080650988966\n",
            "Epoch 3, Batch 185, Loss: 0.0003684847033582628\n",
            "Epoch 3, Batch 186, Loss: 0.0002930318587459624\n",
            "Epoch 3, Batch 187, Loss: 0.00045667390804737806\n",
            "Epoch 3, Batch 188, Loss: 0.00032338345772586763\n",
            "Epoch 3, Batch 189, Loss: 0.0003312338376417756\n",
            "Epoch 3, Batch 190, Loss: 0.0003304637793917209\n",
            "Epoch 3, Batch 191, Loss: 0.0009825096931308508\n",
            "Epoch 3, Batch 192, Loss: 0.00030215171864256263\n",
            "Epoch 3, Batch 193, Loss: 0.00031437244615517557\n",
            "Epoch 3, Batch 194, Loss: 0.0003348346217535436\n",
            "Epoch 3, Batch 195, Loss: 0.00035195518285036087\n",
            "Epoch 3, Batch 196, Loss: 0.00032558702514506876\n",
            "Epoch 3, Batch 197, Loss: 0.0003014535759575665\n",
            "Epoch 3, Batch 198, Loss: 0.12910382449626923\n",
            "Epoch 3, Batch 199, Loss: 0.00031997449696063995\n",
            "Epoch 3, Batch 200, Loss: 0.00027716264594346285\n",
            "Epoch 3, Batch 201, Loss: 0.00030613940907642245\n",
            "Epoch 3, Batch 202, Loss: 0.0067681800574064255\n",
            "Epoch 3, Batch 203, Loss: 0.0003493336262181401\n",
            "Epoch 3, Batch 204, Loss: 0.00039180717431008816\n",
            "Epoch 3, Batch 205, Loss: 0.00032591415219940245\n",
            "Epoch 3, Batch 206, Loss: 0.00037781879655085504\n",
            "Epoch 3, Batch 207, Loss: 0.00032770767575129867\n",
            "Epoch 3, Batch 208, Loss: 0.0003111133410129696\n",
            "Epoch 3, Batch 209, Loss: 0.00032091952743940055\n",
            "Epoch 3, Batch 210, Loss: 0.0003225674736313522\n",
            "Epoch 3, Batch 211, Loss: 0.00032928813016042113\n",
            "Epoch 3, Batch 212, Loss: 0.0003471308737061918\n",
            "Epoch 3, Batch 213, Loss: 0.00032113128690980375\n",
            "Epoch 3, Batch 214, Loss: 0.00035085369017906487\n",
            "Epoch 3, Batch 215, Loss: 0.0004399543395265937\n",
            "Epoch 3, Batch 216, Loss: 0.0003233914321754128\n",
            "Epoch 3, Batch 217, Loss: 0.0003220168291591108\n",
            "Epoch 3, Batch 218, Loss: 0.0003546711814124137\n",
            "Epoch 3, Batch 219, Loss: 0.00043035740964114666\n",
            "Epoch 3, Batch 220, Loss: 0.0003551372792571783\n",
            "Epoch 3, Batch 221, Loss: 0.00034536566818132997\n",
            "Epoch 3, Batch 222, Loss: 0.00043143201037310064\n",
            "Epoch 3, Batch 223, Loss: 0.00036950421053916216\n",
            "Epoch 3, Batch 224, Loss: 0.0003213650779798627\n",
            "Epoch 3, Batch 225, Loss: 0.0003436072147451341\n",
            "Epoch 3, Batch 226, Loss: 0.00034389449865557253\n",
            "Epoch 3, Batch 227, Loss: 0.00033674592850729823\n",
            "Epoch 3, Batch 228, Loss: 0.00035690146614797413\n",
            "Epoch 3, Batch 229, Loss: 0.00044446258107200265\n",
            "Epoch 3, Batch 230, Loss: 0.0003255249175708741\n",
            "Epoch 3, Batch 231, Loss: 0.00033142525353468955\n",
            "Epoch 3, Batch 232, Loss: 0.0003371878410689533\n",
            "Epoch 3, Batch 233, Loss: 0.00031086168019101024\n",
            "Epoch 3, Batch 234, Loss: 0.0003242077073082328\n",
            "Epoch 3, Batch 235, Loss: 0.00035514376941137016\n",
            "Epoch 3, Batch 236, Loss: 0.0003245104744564742\n",
            "Epoch 3, Batch 237, Loss: 0.00031390367075800896\n",
            "Epoch 3, Batch 238, Loss: 0.00035178352845832705\n",
            "Epoch 3, Batch 239, Loss: 0.00031703233253210783\n",
            "Epoch 3, Batch 240, Loss: 0.0003104321949649602\n",
            "Epoch 3, Batch 241, Loss: 0.00032558594830334187\n",
            "Epoch 3, Batch 242, Loss: 0.0004071836010552943\n",
            "Epoch 3, Batch 243, Loss: 0.00031542795477434993\n",
            "Epoch 3, Batch 244, Loss: 0.00031940219923853874\n",
            "Epoch 3, Batch 245, Loss: 0.0003289611195214093\n",
            "Epoch 3, Batch 246, Loss: 0.00031445984495803714\n",
            "Epoch 3, Batch 247, Loss: 0.0003072407271247357\n",
            "Epoch 3, Batch 248, Loss: 0.0005049569299444556\n",
            "Epoch 3, Batch 249, Loss: 0.0003034561814274639\n",
            "Epoch 3, Batch 250, Loss: 0.0003001622390002012\n",
            "Epoch 3, Batch 251, Loss: 0.0002854984486475587\n",
            "Epoch 3, Batch 252, Loss: 0.00034395954571664333\n",
            "Epoch 3, Batch 253, Loss: 0.0003276239149272442\n",
            "Epoch 3, Batch 254, Loss: 0.0003178660408593714\n",
            "Epoch 3, Batch 255, Loss: 0.00038616673555225134\n",
            "Epoch 3, Batch 256, Loss: 0.0004907299298793077\n",
            "Epoch 3, Batch 257, Loss: 0.00030142886680550873\n",
            "Epoch 3, Batch 258, Loss: 0.0002892817428801209\n",
            "Epoch 3, Batch 259, Loss: 0.0003025958430953324\n",
            "Epoch 3, Batch 260, Loss: 0.00031203520484268665\n",
            "Epoch 3, Batch 261, Loss: 0.00033742524101398885\n",
            "Epoch 3, Batch 262, Loss: 0.00029399909544736147\n",
            "Epoch 3, Batch 263, Loss: 0.00028399957227520645\n",
            "Epoch 3, Batch 264, Loss: 0.00027316820342093706\n",
            "Epoch 3, Batch 265, Loss: 0.0002925365115515888\n",
            "Epoch 3, Batch 266, Loss: 0.0002753159205894917\n",
            "Epoch 3, Batch 267, Loss: 0.0002897049707826227\n",
            "Epoch 3, Batch 268, Loss: 0.0002793921157717705\n",
            "Epoch 3, Batch 269, Loss: 0.0002892911434173584\n",
            "Epoch 3, Batch 270, Loss: 0.00029607536271214485\n",
            "Epoch 3, Batch 271, Loss: 0.0002775039756670594\n",
            "Epoch 3, Batch 272, Loss: 0.0002803862444125116\n",
            "Epoch 3, Batch 273, Loss: 0.00028104972443543375\n",
            "Epoch 3, Batch 274, Loss: 0.0002656331053003669\n",
            "Epoch 3, Batch 275, Loss: 0.00027239552582614124\n",
            "Epoch 3, Batch 276, Loss: 0.0002700362238101661\n",
            "Epoch 3, Batch 277, Loss: 0.00029488676227629185\n",
            "Epoch 3, Batch 278, Loss: 0.00030584001797251403\n",
            "Epoch 3, Batch 279, Loss: 0.0002907676389440894\n",
            "Epoch 3, Batch 280, Loss: 0.00029049362638033926\n",
            "Epoch 3, Batch 281, Loss: 0.0002705015940591693\n",
            "Epoch 3, Batch 282, Loss: 0.00028366519836708903\n",
            "Epoch 3, Batch 283, Loss: 0.0002628914953675121\n",
            "Epoch 3, Batch 284, Loss: 0.0003142902278341353\n",
            "Epoch 3, Batch 285, Loss: 0.00026883251848630607\n",
            "Epoch 3, Batch 286, Loss: 0.0016129701398313046\n",
            "Epoch 3, Batch 287, Loss: 0.00031292616040445864\n",
            "Epoch 3, Batch 288, Loss: 0.0002914327778853476\n",
            "Epoch 3, Batch 289, Loss: 0.00033231411362066865\n",
            "Epoch 3, Batch 290, Loss: 0.0002886311849579215\n",
            "Epoch 3, Batch 291, Loss: 0.00026079892995767295\n",
            "Epoch 3, Batch 292, Loss: 0.000275506783509627\n",
            "Epoch 3, Batch 293, Loss: 0.00026949343737214804\n",
            "Epoch 3, Batch 294, Loss: 0.0002575834223534912\n",
            "Epoch 3, Batch 295, Loss: 0.0002782733354251832\n",
            "Epoch 3, Batch 296, Loss: 0.00025746977189555764\n",
            "Epoch 3, Batch 297, Loss: 0.0002835530904121697\n",
            "Epoch 3, Batch 298, Loss: 0.0002637662983033806\n",
            "Epoch 3, Batch 299, Loss: 0.0003858922282233834\n",
            "Epoch 3, Batch 300, Loss: 0.000249597302172333\n",
            "Epoch 3, Batch 301, Loss: 0.0002973996743094176\n",
            "Epoch 3, Batch 302, Loss: 0.00027906778268516064\n",
            "Epoch 3, Batch 303, Loss: 0.00027042574947699904\n",
            "Epoch 3, Batch 304, Loss: 0.00025643198750913143\n",
            "Epoch 3, Batch 305, Loss: 0.0005611397791653872\n",
            "Epoch 3, Batch 306, Loss: 0.0002911899355240166\n",
            "Epoch 3, Batch 307, Loss: 0.0002638142032083124\n",
            "Epoch 3, Batch 308, Loss: 0.00026375395827926695\n",
            "Epoch 3, Batch 309, Loss: 0.0002736716705840081\n",
            "Epoch 3, Batch 310, Loss: 0.00026983802672475576\n",
            "Epoch 3, Batch 311, Loss: 0.0002833456383086741\n",
            "Epoch 3, Batch 312, Loss: 0.00041473782039247453\n",
            "Epoch 3, Batch 313, Loss: 0.0002520435373298824\n",
            "Epoch 3, Batch 314, Loss: 0.00026995205553248525\n",
            "Epoch 3, Batch 315, Loss: 0.0002994484966620803\n",
            "Epoch 3, Batch 316, Loss: 0.00027251295978203416\n",
            "Epoch 3, Batch 317, Loss: 0.00030367803992703557\n",
            "Epoch 3, Batch 318, Loss: 0.00025736732641234994\n",
            "Epoch 3, Batch 319, Loss: 0.00041224848246201873\n",
            "Epoch 3, Batch 320, Loss: 0.00025310571072623134\n",
            "Epoch 3, Batch 321, Loss: 0.00025049870600923896\n",
            "Epoch 3, Batch 322, Loss: 0.00025792777887545526\n",
            "Epoch 3, Batch 323, Loss: 0.00025505374651402235\n",
            "Epoch 3, Batch 324, Loss: 0.0002974372182507068\n",
            "Epoch 3, Batch 325, Loss: 0.0002520980197004974\n",
            "Epoch 3, Batch 326, Loss: 0.0002637072466313839\n",
            "Epoch 3, Batch 327, Loss: 0.0003665722906589508\n",
            "Epoch 3, Batch 328, Loss: 0.0002489800099283457\n",
            "Epoch 3, Batch 329, Loss: 0.00025644840206950903\n",
            "Epoch 3, Batch 330, Loss: 0.0002543266164138913\n",
            "Epoch 3, Batch 331, Loss: 0.0002513996441848576\n",
            "Epoch 3, Batch 332, Loss: 0.00024276156909763813\n",
            "Epoch 3, Batch 333, Loss: 0.00026697327848523855\n",
            "Epoch 3, Batch 334, Loss: 0.0002564728492870927\n",
            "Epoch 3, Batch 335, Loss: 0.0002407928986940533\n",
            "Epoch 3, Batch 336, Loss: 0.0002466582809574902\n",
            "Epoch 3, Batch 337, Loss: 0.00025235695648007095\n",
            "Epoch 3, Batch 338, Loss: 0.0002352236188016832\n",
            "Epoch 3, Batch 339, Loss: 0.000233317565289326\n",
            "Epoch 3, Batch 340, Loss: 0.0002611837408039719\n",
            "Epoch 3, Batch 341, Loss: 0.00024340386153198779\n",
            "Epoch 3, Batch 342, Loss: 0.0002571003860794008\n",
            "Epoch 3, Batch 343, Loss: 0.00025372777599841356\n",
            "Epoch 3, Batch 344, Loss: 0.00023315644648391753\n",
            "Epoch 3, Batch 345, Loss: 0.00023170417989604175\n",
            "Epoch 3, Batch 346, Loss: 0.00025561757502146065\n",
            "Epoch 3, Batch 347, Loss: 0.0002475201617926359\n",
            "Epoch 3, Batch 348, Loss: 0.00024201962514780462\n",
            "Epoch 3, Batch 349, Loss: 0.00023740236065350473\n",
            "Epoch 3, Batch 350, Loss: 0.00024105145712383091\n",
            "Epoch 3, Batch 351, Loss: 0.00027346654678694904\n",
            "Epoch 3, Batch 352, Loss: 0.00022530928254127502\n",
            "Epoch 3, Batch 353, Loss: 0.0003736653015948832\n",
            "Epoch 3, Batch 354, Loss: 0.0002573867095634341\n",
            "Epoch 3, Batch 355, Loss: 0.00025046124937944114\n",
            "Epoch 3, Batch 356, Loss: 0.00023632764350622892\n",
            "Epoch 3, Batch 357, Loss: 0.00023330953263211995\n",
            "Epoch 3, Batch 358, Loss: 0.0002616715501062572\n",
            "Epoch 3, Batch 359, Loss: 0.0002431253087706864\n",
            "Epoch 3, Batch 360, Loss: 0.00023451268498320132\n",
            "Epoch 3, Batch 361, Loss: 0.00028404948534443974\n",
            "Epoch 3, Batch 362, Loss: 0.00024023183505050838\n",
            "Epoch 3, Batch 363, Loss: 0.00040351581992581487\n",
            "Epoch 3, Batch 364, Loss: 0.00024116737768054008\n",
            "Epoch 3, Batch 365, Loss: 0.0002354786265641451\n",
            "Epoch 3, Batch 366, Loss: 0.00023675174452364445\n",
            "Epoch 3, Batch 367, Loss: 0.0002869103627745062\n",
            "Epoch 3, Batch 368, Loss: 0.00022678564710076898\n",
            "Epoch 3, Batch 369, Loss: 0.0002318351180292666\n",
            "Epoch 3, Batch 370, Loss: 0.0008332039578817785\n",
            "Epoch 3, Batch 371, Loss: 0.00025051849661394954\n",
            "Epoch 3, Batch 372, Loss: 0.0004579824162647128\n",
            "Epoch 3, Batch 373, Loss: 0.00029099464882165194\n",
            "Epoch 3, Batch 374, Loss: 0.000262694142293185\n",
            "Epoch 3, Batch 375, Loss: 0.00023510036407969892\n",
            "Epoch 3, Batch 376, Loss: 0.00021521378948818892\n",
            "Epoch 3, Batch 377, Loss: 0.00023804376542102545\n",
            "Epoch 3, Batch 378, Loss: 0.00022178899962455034\n",
            "Epoch 3, Batch 379, Loss: 0.0002980489225592464\n",
            "Epoch 3, Batch 380, Loss: 0.00022268206521403044\n",
            "Epoch 3, Batch 381, Loss: 0.00023396528558805585\n",
            "Epoch 3, Batch 382, Loss: 0.00023911037715151906\n",
            "Epoch 3, Batch 383, Loss: 0.00023762155615258962\n",
            "Epoch 3, Batch 384, Loss: 0.00024028615735005587\n",
            "Epoch 3, Batch 385, Loss: 0.00022180151427164674\n",
            "Epoch 3, Batch 386, Loss: 0.000217437744140625\n",
            "Epoch 3, Batch 387, Loss: 0.00022586146951653063\n",
            "Epoch 3, Batch 388, Loss: 0.00024360402312595397\n",
            "Epoch 3, Batch 389, Loss: 0.00023350374249275774\n",
            "Epoch 3, Batch 390, Loss: 0.00022707629250362515\n",
            "Epoch 3, Batch 391, Loss: 0.0002399628283455968\n",
            "Epoch 3, Batch 392, Loss: 0.0002207127690780908\n",
            "Epoch 3, Batch 393, Loss: 0.0002318991464562714\n",
            "Epoch 3, Batch 394, Loss: 0.00021538730652537197\n",
            "Epoch 3, Batch 395, Loss: 0.00022907881066203117\n",
            "Epoch 3, Batch 396, Loss: 0.0002318186016054824\n",
            "Epoch 3, Batch 397, Loss: 0.00023318015155382454\n",
            "Epoch 3, Batch 398, Loss: 0.00022827961947768927\n",
            "Epoch 3, Batch 399, Loss: 0.00021570935496129096\n",
            "Epoch 3, Batch 400, Loss: 0.00023321656044572592\n",
            "Epoch 3, Batch 401, Loss: 0.00023175988462753594\n",
            "Epoch 3, Batch 402, Loss: 0.0002170899824704975\n",
            "Epoch 3, Batch 403, Loss: 0.00020670969388447702\n",
            "Epoch 3, Batch 404, Loss: 0.000214261410292238\n",
            "Epoch 3, Batch 405, Loss: 0.00020934094209223986\n",
            "Epoch 3, Batch 406, Loss: 0.00022412589169107378\n",
            "Epoch 3, Batch 407, Loss: 0.00029632216319441795\n",
            "Epoch 3, Batch 408, Loss: 0.0002238854649476707\n",
            "Epoch 3, Batch 409, Loss: 0.00023423648963216692\n",
            "Epoch 3, Batch 410, Loss: 0.00022588895808439702\n",
            "Epoch 3, Batch 411, Loss: 0.00022738358529750258\n",
            "Epoch 3, Batch 412, Loss: 0.00022015246213413775\n",
            "Epoch 3, Batch 413, Loss: 0.00024019207921810448\n",
            "Epoch 3, Batch 414, Loss: 0.00023573663202114403\n",
            "Epoch 3, Batch 415, Loss: 0.00021630979608744383\n",
            "Epoch 3, Batch 416, Loss: 0.0002135977119905874\n",
            "Epoch 3, Batch 417, Loss: 0.00023873680038377643\n",
            "Epoch 3, Batch 418, Loss: 0.00021982190082781017\n",
            "Epoch 3, Batch 419, Loss: 0.0002822231035679579\n",
            "Epoch 3, Batch 420, Loss: 0.0002183012547902763\n",
            "Epoch 3, Batch 421, Loss: 0.00020931422477588058\n",
            "Epoch 3, Batch 422, Loss: 0.00023592266370542347\n",
            "Epoch 3, Batch 423, Loss: 0.00023316424631047994\n",
            "Epoch 3, Batch 424, Loss: 0.00022175585036166012\n",
            "Epoch 3, Batch 425, Loss: 0.00021269261196721345\n",
            "Epoch 3, Batch 426, Loss: 0.00020967253658454865\n",
            "Epoch 3, Batch 427, Loss: 0.00021346948051359504\n",
            "Epoch 3, Batch 428, Loss: 0.00021729402942582965\n",
            "Epoch 3, Batch 429, Loss: 0.0002247697557322681\n",
            "Epoch 3, Batch 430, Loss: 0.00023240972950588912\n",
            "Epoch 3, Batch 431, Loss: 0.00024297734489664435\n",
            "Epoch 3, Batch 432, Loss: 0.00021855669911019504\n",
            "Epoch 3, Batch 433, Loss: 0.0002030842297244817\n",
            "Epoch 3, Batch 434, Loss: 0.00021518573339562863\n",
            "Epoch 3, Batch 435, Loss: 0.00022804005129728466\n",
            "Epoch 3, Batch 436, Loss: 0.0002106031315634027\n",
            "Epoch 3, Batch 437, Loss: 0.0002173918765038252\n",
            "Epoch 3, Batch 438, Loss: 0.0002171801752410829\n",
            "Epoch 3, Batch 439, Loss: 0.00020813095034100115\n",
            "Epoch 3, Batch 440, Loss: 0.00021949710207991302\n",
            "Epoch 3, Batch 441, Loss: 0.00021266451221890748\n",
            "Epoch 3, Batch 442, Loss: 0.00021085509797558188\n",
            "Epoch 3, Batch 443, Loss: 0.00022474859724752605\n",
            "Epoch 3, Batch 444, Loss: 0.00020556931849569082\n",
            "Epoch 3, Batch 445, Loss: 0.0002190614613937214\n",
            "Epoch 3, Batch 446, Loss: 0.0001990774180740118\n",
            "Epoch 3, Batch 447, Loss: 0.0002112163492711261\n",
            "Epoch 3, Batch 448, Loss: 0.00028943317010998726\n",
            "Epoch 3, Batch 449, Loss: 0.0013436119770631194\n",
            "Epoch 3, Batch 450, Loss: 0.0002076740493066609\n",
            "Epoch 3, Batch 451, Loss: 0.00021429132902994752\n",
            "Epoch 3, Batch 452, Loss: 0.00024659762857481837\n",
            "Epoch 3, Batch 453, Loss: 0.00021038144768681377\n",
            "Epoch 3, Batch 454, Loss: 0.0002107676991727203\n",
            "Epoch 3, Batch 455, Loss: 0.0002047722227871418\n",
            "Epoch 3, Batch 456, Loss: 0.00021554544218815863\n",
            "Epoch 3, Batch 457, Loss: 0.00019961714860983193\n",
            "Epoch 3, Batch 458, Loss: 0.00021992839174345136\n",
            "Epoch 3, Batch 459, Loss: 0.0002790879807434976\n",
            "Epoch 3, Batch 460, Loss: 0.00020290803513489664\n",
            "Epoch 3, Batch 461, Loss: 0.00019802979659289122\n",
            "Epoch 3, Batch 462, Loss: 0.00020552353817038238\n",
            "Epoch 3, Batch 463, Loss: 0.0002116043760906905\n",
            "Epoch 3, Batch 464, Loss: 0.00020819864585064352\n",
            "Epoch 3, Batch 465, Loss: 0.00026918298681266606\n",
            "Epoch 3, Batch 466, Loss: 0.00038709992077201605\n",
            "Epoch 3, Batch 467, Loss: 0.00021172899869270623\n",
            "Epoch 3, Batch 468, Loss: 0.0002000037202378735\n",
            "Epoch 3, Batch 469, Loss: 0.00019987751147709787\n",
            "Epoch 3, Batch 470, Loss: 0.0001982936228159815\n",
            "Epoch 3, Batch 471, Loss: 0.00020198221318423748\n",
            "Epoch 3, Batch 472, Loss: 0.00022812103270553052\n",
            "Epoch 3, Batch 473, Loss: 0.0002087891916744411\n",
            "Epoch 3, Batch 474, Loss: 0.00029817534959875047\n",
            "Epoch 3, Batch 475, Loss: 0.00020049801969435066\n",
            "Epoch 3, Batch 476, Loss: 0.00020813541777897626\n",
            "Epoch 3, Batch 477, Loss: 0.00019709051412064582\n",
            "Epoch 3, Batch 478, Loss: 0.00020409924036357552\n",
            "Epoch 3, Batch 479, Loss: 0.0001986234710784629\n",
            "Epoch 3, Batch 480, Loss: 0.000203914096346125\n",
            "Epoch 3, Batch 481, Loss: 0.00019496670574881136\n",
            "Epoch 3, Batch 482, Loss: 0.00020176837278995663\n",
            "Epoch 3, Batch 483, Loss: 0.0001958327484317124\n",
            "Epoch 3, Batch 484, Loss: 0.00020093622151762247\n",
            "Epoch 3, Batch 485, Loss: 0.0002015214558923617\n",
            "Epoch 3, Batch 486, Loss: 0.00021400986588560045\n",
            "Epoch 3, Batch 487, Loss: 0.00018780665413942188\n",
            "Epoch 3, Batch 488, Loss: 0.0001964555704034865\n",
            "Epoch 3, Batch 489, Loss: 0.00020416142069734633\n",
            "Epoch 3, Batch 490, Loss: 0.00019969537970609963\n",
            "Epoch 3, Batch 491, Loss: 0.00019832502584904432\n",
            "Epoch 3, Batch 492, Loss: 0.00018145737703889608\n"
          ]
        }
      ],
      "source": [
        "# Define the loss function\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# Define the metrics\n",
        "metrics = [tf.keras.metrics.BinaryAccuracy('accuracy')]\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.keras.optimizers.AdamW(learning_rate=2e-5)\n",
        "\n",
        "# Define a training step function\n",
        "@tf.function\n",
        "def train_step(inputs, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inputs, training=True)['logits']\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  metrics[0].update_state(labels, tf.sigmoid(predictions))  # Update accuracy metric\n",
        "  return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 3\n",
        "batch_size = 64\n",
        "for epoch in range(epochs):\n",
        "  for i in range(0, len(train_input_ids), batch_size):\n",
        "    batch_input_ids = train_input_ids[i:i + batch_size]\n",
        "    batch_attention_masks = train_attention_masks[i:i + batch_size]\n",
        "    batch_labels = train_labels[i:i + batch_size]\n",
        "    loss = train_step({'input_ids': batch_input_ids, 'attention_mask': batch_attention_masks}, batch_labels)\n",
        "    print(f\"Epoch {epoch + 1}, Batch {i // batch_size + 1}, Loss: {loss.numpy()}\")\n",
        "\n",
        "  # Reset metrics for the next epoch\n",
        "  metrics[0].reset_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca_fFDsR0HFN",
        "outputId": "04806fa8-6603-4c5c-ccda-2bec03f2d3ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "211/211 [==============================] - 64s 272ms/step\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "predictions = model.predict({'input_ids': test_input_ids, 'attention_mask': test_attention_masks})\n",
        "predicted_labels = (tf.sigmoid(predictions.logits) > 0.5).numpy().astype(int).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6h5CPRhk0JYx"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "precision = precision_score(y_test, predicted_labels)\n",
        "recall = recall_score(y_test, predicted_labels)\n",
        "f1 = f1_score(y_test, predicted_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYyHrng_0MgA",
        "outputId": "2fc500a7-39d6-4d47-9567-0092f70e892b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9986636971046771\n",
            "Precision: 0.9990651293237769\n",
            "Recall: 0.99813200498132\n",
            "F1-score: 0.9985983491667965\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
