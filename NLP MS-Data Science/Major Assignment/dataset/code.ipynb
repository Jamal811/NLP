{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\PC\n",
      "[nltk_data]     VISION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\PC\n",
      "[nltk_data]     VISION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Sample data:\n",
      "                                                   title  \\\n",
      "5769   Highlights: The Trump presidency on February 1...   \n",
      "24756   George Takei NAILS Republican Hypocrisy On Tr...   \n",
      "24283   Trump Just Signed Away Our Environment, And A...   \n",
      "16070  New Zealand PM says ban on foreign home buyers...   \n",
      "39371  WATCH: RACIST RAPPER WHO HUNG WHITE KID In Lat...   \n",
      "\n",
      "                                                    text       subject  \\\n",
      "5769   (Reuters) - Highlights of the day for U.S. Pre...  politicsNews   \n",
      "24756  Donald Trump insists that he doesn t have to d...          News   \n",
      "24283  Donald Trump betrayed Americans on Tuesday by ...          News   \n",
      "16070  WELLINGTON (Reuters) - New Zealand Prime Minis...     worldnews   \n",
      "39371  Rapper XXXTentacion released a controversial v...     left-news   \n",
      "\n",
      "                    date  label  \n",
      "5769   February 1, 2017       1  \n",
      "24756  December 20, 2016      0  \n",
      "24283   January 24, 2017      0  \n",
      "16070  October 31, 2017       1  \n",
      "39371       Sep 17, 2017      0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, Dense, Dropout\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the datasets\n",
    "true_news_path = './True.csv'\n",
    "fake_news_path = './Fake.csv'\n",
    "\n",
    "true_news_df = pd.read_csv(true_news_path)\n",
    "fake_news_df = pd.read_csv(fake_news_path)\n",
    "\n",
    "# Add labels: 1 for True, 0 for Fake\n",
    "true_news_df['label'] = 1\n",
    "fake_news_df['label'] = 0\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([true_news_df, fake_news_df], ignore_index=True)\n",
    "print(\"Dataset loaded. Sample data:\")\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Libraries and Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Preprocessing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "\n",
    "# Convert text to sequences and pad them\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "# Extract labels\n",
    "labels = df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 31428, Validation set size: 6735, Test set size: 6735\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(padded_sequences, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}, Validation set size: {len(X_val)}, Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Pretrained Embeddings Setup\n",
    "Word2Vec, FastText, and GloVe Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pretrained embeddings\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "# Download GloVe embeddings and load them\n",
    "import requests, zipfile, os\n",
    "\n",
    "glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "glove_zip_path = \"glove.6B.zip\"\n",
    "\n",
    "response = requests.get(glove_url)\n",
    "with open(glove_zip_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "with zipfile.ZipFile(glove_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "print(\"GloVe embeddings downloaded and extracted.\")\n",
    "\n",
    "# Load GloVe (100-dimensional)\n",
    "def load_glove_embeddings(filepath):\n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(\"glove.6B.100d.txt\")\n",
    "\n",
    "# Create embedding matrices\n",
    "def create_embedding_matrix(embeddings, tokenizer, dim):\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in embeddings:\n",
    "            embedding_matrix[i] = embeddings[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "word2vec_matrix = create_embedding_matrix(word2vec_model, tokenizer, 300)\n",
    "fasttext_matrix = create_embedding_matrix(fasttext_model, tokenizer, 300)\n",
    "glove_matrix = create_embedding_matrix(glove_embeddings, tokenizer, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Model Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_matrix):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.4),\n",
    "        GRU(64),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate with different embeddings\n",
    "pretrained_matrices = {\n",
    "    \"Word2Vec\": word2vec_matrix,\n",
    "    \"FastText\": fasttext_matrix,\n",
    "    \"GloVe\": glove_matrix\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, matrix in pretrained_matrices.items():\n",
    "    print(f\"Training model with {name} embeddings...\")\n",
    "    model = create_model(matrix)\n",
    "    model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_val, y_val))\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    results[name] = (loss, accuracy)\n",
    "    print(f\"{name} - Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Custom Embeddings Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "# Train custom Word2Vec embeddings\n",
    "custom_word2vec = Word2Vec(sentences=df['cleaned_text'].apply(lambda x: x.split()), vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Train custom FastText embeddings\n",
    "custom_fasttext = FastText(sentences=df['cleaned_text'].apply(lambda x: x.split()), vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Train custom GloVe embeddings\n",
    "corpus = Corpus()\n",
    "corpus.fit(df['cleaned_text'].apply(lambda x: x.split()), window=5)\n",
    "custom_glove = Glove(no_components=100, learning_rate=0.05)\n",
    "custom_glove.fit(corpus.matrix, epochs=10, no_threads=4, verbose=True)\n",
    "custom_glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "# Save models\n",
    "custom_word2vec.save(\"custom_word2vec.model\")\n",
    "custom_fasttext.save(\"custom_fasttext.model\")\n",
    "custom_glove.save(\"custom_glove.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Custom Embeddings Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices from custom embeddings\n",
    "custom_word2vec_matrix = create_embedding_matrix(custom_word2vec.wv, tokenizer, 100)\n",
    "custom_fasttext_matrix = create_embedding_matrix(custom_fasttext.wv, tokenizer, 100)\n",
    "custom_glove_matrix = create_embedding_matrix(custom_glove.dictionary, tokenizer, 100)\n",
    "\n",
    "custom_matrices = {\n",
    "    \"Custom Word2Vec\": custom_word2vec_matrix,\n",
    "    \"Custom FastText\": custom_fasttext_matrix,\n",
    "    \"Custom GloVe\": custom_glove_matrix\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "for name, matrix in custom_matrices.items():\n",
    "    print(f\"Training model with {name} embeddings...\")\n",
    "    model = create_model(matrix)\n",
    "    model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_val, y_val))\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    results[name] = (loss, accuracy)\n",
    "    print(f\"{name} - Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Model Architecture Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, Dense, Dropout\n",
    "\n",
    "# Create LSTM Model\n",
    "def create_lstm_model(embedding_matrix):\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=embedding_matrix.shape[0], \n",
    "            output_dim=embedding_matrix.shape[1], \n",
    "            weights=[embedding_matrix], \n",
    "            trainable=False\n",
    "        ),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.4),\n",
    "        LSTM(64),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create GRU Model\n",
    "def create_gru_model(embedding_matrix):\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=embedding_matrix.shape[0], \n",
    "            output_dim=embedding_matrix.shape[1], \n",
    "            weights=[embedding_matrix], \n",
    "            trainable=False\n",
    "        ),\n",
    "        GRU(128, return_sequences=True),\n",
    "        Dropout(0.4),\n",
    "        GRU(64),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create Bi-LSTM Model\n",
    "def create_bilstm_model(embedding_matrix):\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=embedding_matrix.shape[0], \n",
    "            output_dim=embedding_matrix.shape[1], \n",
    "            weights=[embedding_matrix], \n",
    "            trainable=False\n",
    "        ),\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.4),\n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of pretrained embedding matrices\n",
    "pretrained_matrices = {\n",
    "    \"Word2Vec\": word2vec_matrix,\n",
    "    \"FastText\": fasttext_matrix,\n",
    "    \"GloVe\": glove_matrix\n",
    "}\n",
    "\n",
    "# Train and evaluate each architecture with each embedding\n",
    "results = {}\n",
    "for name, matrix in pretrained_matrices.items():\n",
    "    print(f\"\\nUsing {name} embeddings:\")\n",
    "    \n",
    "    # Train and evaluate LSTM\n",
    "    print(\"Training LSTM...\")\n",
    "    lstm_model = create_lstm_model(matrix)\n",
    "    lstm_model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_val, y_val))\n",
    "    lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "    print(f\"LSTM - Loss: {lstm_loss}, Accuracy: {lstm_accuracy}\")\n",
    "    \n",
    "    # Train and evaluate GRU\n",
    "    print(\"Training GRU...\")\n",
    "    gru_model = create_gru_model(matrix)\n",
    "    gru_model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_val, y_val))\n",
    "    gru_loss, gru_accuracy = gru_model.evaluate(X_test, y_test)\n",
    "    print(f\"GRU - Loss: {gru_loss}, Accuracy: {gru_accuracy}\")\n",
    "    \n",
    "    # Train and evaluate Bi-LSTM\n",
    "    print(\"Training Bi-LSTM...\")\n",
    "    bilstm_model = create_bilstm_model(matrix)\n",
    "    bilstm_model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_val, y_val))\n",
    "    bilstm_loss, bilstm_accuracy = bilstm_model.evaluate(X_test, y_test)\n",
    "    print(f\"Bi-LSTM - Loss: {bilstm_loss}, Accuracy: {bilstm_accuracy}\")\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"LSTM\": (lstm_loss, lstm_accuracy),\n",
    "        \"GRU\": (gru_loss, gru_accuracy),\n",
    "        \"Bi-LSTM\": (bilstm_loss, bilstm_accuracy)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results for each embedding and architecture\n",
    "for embedding, metrics in results.items():\n",
    "    print(f\"\\nResults for {embedding} embeddings:\")\n",
    "    for model_type, (loss, accuracy) in metrics.items():\n",
    "        print(f\"{model_type} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Using BERT for Transformer-Based Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Prepare the dataset for BERT\n",
    "def prepare_bert_input(texts, tokenizer, max_length=128):\n",
    "    input_ids, attention_masks = [], []\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        input_ids.append(encoded[\"input_ids\"])\n",
    "        attention_masks.append(encoded[\"attention_mask\"])\n",
    "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)\n",
    "\n",
    "# Tokenize train, validation, and test datasets\n",
    "train_input_ids, train_attention_masks = prepare_bert_input(X_train, bert_tokenizer)\n",
    "val_input_ids, val_attention_masks = prepare_bert_input(X_val, bert_tokenizer)\n",
    "test_input_ids, test_attention_masks = prepare_bert_input(X_test, bert_tokenizer)\n",
    "\n",
    "# Reshape labels\n",
    "train_labels = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "val_labels = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "test_labels = tf.convert_to_tensor(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10: Fine-Tuning BERT for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define BERT-based classification model\n",
    "def create_bert_model():\n",
    "    input_ids = Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[1]  # Pooled output\n",
    "    dense = Dense(64, activation=\"relu\")(bert_output)\n",
    "    output = Dense(1, activation=\"sigmoid\")(dense)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=2e-5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Instantiate and train the model\n",
    "bert_classifier = create_bert_model()\n",
    "\n",
    "bert_classifier.fit(\n",
    "    x={\"input_ids\": train_input_ids, \"attention_mask\": train_attention_masks},\n",
    "    y=train_labels,\n",
    "    validation_data=({\"input_ids\": val_input_ids, \"attention_mask\": val_attention_masks}, val_labels),\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11: Evaluate BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the BERT model on the test dataset\n",
    "test_loss, test_accuracy = bert_classifier.evaluate(\n",
    "    x={\"input_ids\": test_input_ids, \"attention_mask\": test_attention_masks},\n",
    "    y=test_labels\n",
    ")\n",
    "\n",
    "print(f\"BERT Test Loss: {test_loss}\")\n",
    "print(f\"BERT Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 12: Comparative Analysis of All Approaches\n",
    "To summarize and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results in a dictionary\n",
    "results[\"BERT\"] = (test_loss, test_accuracy)\n",
    "\n",
    "# Print all results\n",
    "for embedding_type, (loss, accuracy) in results.items():\n",
    "    print(f\"{embedding_type}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Save Models for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the BERT model\n",
    "bert_classifier.save(\"bert_fake_news_classifier.h5\")\n",
    "\n",
    "# Save tokenizer for reuse\n",
    "import pickle\n",
    "with open(\"bert_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bert_tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 13: Additional Transformer-Based Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Prepare Input Using DistilBERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load DistilBERT tokenizer and model\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "distilbert_model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Function to prepare input for DistilBERT\n",
    "def prepare_distilbert_input(texts, tokenizer, max_length=128):\n",
    "    input_ids, attention_masks = [], []\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        input_ids.append(encoded[\"input_ids\"])\n",
    "        attention_masks.append(encoded[\"attention_mask\"])\n",
    "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)\n",
    "\n",
    "# Prepare the dataset\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(df['cleaned_text'], labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "train_input_ids, train_attention_masks = prepare_distilbert_input(X_train, distilbert_tokenizer)\n",
    "val_input_ids, val_attention_masks = prepare_distilbert_input(X_val, distilbert_tokenizer)\n",
    "test_input_ids, test_attention_masks = prepare_distilbert_input(X_test, distilbert_tokenizer)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "val_labels = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "test_labels = tf.convert_to_tensor(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create DistilBERT Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define a model using DistilBERT embeddings\n",
    "def create_distilbert_model():\n",
    "    input_ids = Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    distilbert_output = distilbert_model(input_ids, attention_mask=attention_mask)[0][:, 0, :]  # CLS token\n",
    "    dense = Dense(64, activation=\"relu\")(distilbert_output)\n",
    "    dropout = Dropout(0.3)(dense)\n",
    "    output = Dense(1, activation=\"sigmoid\")(dropout)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=2e-5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "distilbert_classifier = create_distilbert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Train DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "distilbert_classifier.fit(\n",
    "    x={\"input_ids\": train_input_ids, \"attention_mask\": train_attention_masks},\n",
    "    y=train_labels,\n",
    "    validation_data=({\"input_ids\": val_input_ids, \"attention_mask\": val_attention_masks}, val_labels),\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Evaluate DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = distilbert_classifier.evaluate(\n",
    "    x={\"input_ids\": test_input_ids, \"attention_mask\": test_attention_masks},\n",
    "    y=test_labels\n",
    ")\n",
    "\n",
    "print(f\"DistilBERT Test Loss: {test_loss}\")\n",
    "print(f\"DistilBERT Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DistilBERT model and tokenizer for reuse\n",
    "distilbert_classifier.save(\"distilbert_fake_news_classifier.h5\")\n",
    "with open(\"distilbert_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(distilbert_tokenizer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
